{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mchorse/miniconda3/envs/logan/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m-deduped into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "from neuron_text_simplifier import NeuronTextSimplifier\n",
    "import torch as th\n",
    "\n",
    "device = \"cuda:2\" if th.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "model = HookedTransformer.from_pretrained(model_name, device=device)\n",
    "layer = 1\n",
    "neuron = 1306\n",
    "neurons = [[924],[1306]]\n",
    "simplifier = NeuronTextSimplifier(model, layer, neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-dcbfdc56-c001\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.39.1/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-dcbfdc56-c001\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [\" Yes\", \" Yes\", \"\\n\", \"?\", \"\\\\newline\", \" Yes\", \"\\n\", \"?\", \" \\\"\", \" \\\"\", \" Yes\", \"\\n\"], \"activations\": [[[-0.8059669733047485]], [[-2.660071611404419]], [[0.0]], [[-1.814565658569336]], [[-0.41938138008117676]], [[-3.2553136348724365]], [[0.0]], [[-2.3300936222076416]], [[-0.19896364212036133]], [[-0.8111145496368408]], [[-4.653800010681152]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f61be1f3250>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from circuitsvis.activations import text_neuron_activations\n",
    "text = [\n",
    "    \" Yes Yes\",\n",
    "    \"?\\n Yes\",\n",
    "    \"? \\\" \\\" Yes\",\n",
    "]\n",
    "neuron = 1306\n",
    "noise_level = 1.0\n",
    "\n",
    "# def add_noise_to_text(text, noise_level):\n",
    "#     if isinstance(text, str):\n",
    "#         text = [text]\n",
    "#     text_list = []\n",
    "#     activation_list = []\n",
    "#     for t in text:\n",
    "#         split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "#         tokens = model.to_tokens(t, prepend_bos=False)\n",
    "#         # Add gaussian noise to the input of each word in turn, getting the diff in final neuron's response\n",
    "#         embedded_tokens = self.model.embed(tokens)\n",
    "#         batch_size, seq_size, embedding_size = embedded_tokens.shape\n",
    "#         noise = th.randn(1, embedding_size, device=device)*noise_level\n",
    "#         original = self.embedded_forward(embedded_tokens)[:,-1,self.neuron]\n",
    "#         changed_activations = th.zeros(seq_size, device=device)\n",
    "#         for i in range(seq_size):\n",
    "#             embedded_tokens[:,i,:] += noise\n",
    "#             neuron_response = self.embedded_forward(embedded_tokens)\n",
    "#             changed_activations[i] = neuron_response[:,-1,self.neuron].item()\n",
    "#             embedded_tokens[:,i,:] -= noise\n",
    "#         changed_activations -= original\n",
    "#         text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
    "#         activation_list += changed_activations.tolist() + [0.0]\n",
    "#     activation_list = th.tensor(activation_list).reshape(-1,1,1)\n",
    "#     text_neuron_activations(tokens=text_list, activations=activation_list)\n",
    "\n",
    "simplifier.add_noise_to_text(text, noise_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5115275382995605\n",
      "2.574535846710205\n",
      "4.468333721160889\n",
      "3.9256157875061035\n",
      "0.44166746735572815\n"
     ]
    }
   ],
   "source": [
    "# Add gaussian noise to the input of each word in turn, getting the final neuron's response\n",
    "token = model.to_tokens(\"? \\\" \\\" Yes\")\n",
    "embedded_tokens = simplifier.model.embed(token)\n",
    "# add noise to the first word\n",
    "batch_size, seq_size, embedding_size = embedded_tokens.shape\n",
    "noise_level = 0.1\n",
    "noise = th.randn(1, embedding_size, device=device)*noise_level\n",
    "for i in range(seq_size):\n",
    "    embedded_tokens[:,i,:] += noise\n",
    "    neuron_response = simplifier.embedded_forward(embedded_tokens)\n",
    "    print(neuron_response[:,-1,1306].item())\n",
    "    embedded_tokens[:,i,:] -= noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 2048])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron_response.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m-deduped into HookedTransformer\n",
      "2\n",
      "cuda:2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/neelnanda-io/TransformerLens/blob/main/transformer_lens/HookedTransformer.py#L370\n",
    "from transformer_lens import HookedTransformer\n",
    "model = HookedTransformer.from_pretrained(model_name, device=\"cuda:2\")\n",
    "token = model.to_tokens(\"testing 1 2 3\")\n",
    "logits, cache = model.run_with_cache(token)\n",
    "print(logits.get_device())\n",
    "print(model.cfg.device)\n",
    "print(cache[\"blocks.0.mlp.hook_post\"].get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max & Min logit-diff: 0.02 & -0.01\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-9e60226d-c34b\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.39.1/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-9e60226d-c34b\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [\"testing\", \" 1\", \" 2\", \" 3\", \"\\n\", \" a\", \" b\", \" c\", \"\\n\", \" d\", \" e\", \" f\", \"\\n\", \" g\", \" h\", \"\\n\"], \"activations\": [[[0.005126237869262695]], [[-0.0007123947143554688]], [[0.0049610137939453125]], [[0.014115333557128906]], [[0.0]], [[-0.0004611015319824219]], [[-0.0038213729858398438]], [[0.021770477294921875]], [[0.0]], [[0.0026216506958007812]], [[-0.012768268585205078]], [[-0.0062503814697265625]], [[0.0]], [[-0.004070758819580078]], [[0.004515171051025391]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f9e78c57c70>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jaxtyping import Float, Int\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")\n",
    "from circuitsvis.activations import text_neuron_activations\n",
    "simplifier = NeuronTextSimplifier(model, layer, neuron)\n",
    "def ablate_mlp_neurons(tokens, neurons: th.Tensor):\n",
    "    def mlp_ablation_hook(\n",
    "        value: Float[th.Tensor, \"batch pos d_mlp\"],\n",
    "        hook: HookPoint\n",
    "    ) -> Float[th.Tensor, \"batch pos d_mlp\"]:\n",
    "        value[:, :, neurons] = 0\n",
    "        return value\n",
    "    return model.run_with_hooks(tokens, fwd_hooks=[(f\"blocks.{simplifier.layer}.mlp.hook_post\", mlp_ablation_hook)])\n",
    "def visualize_logit_diff(text, neurons: th.Tensor, setting=\"true_tokens\"):\n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    text_list = []\n",
    "    logit_list = []\n",
    "    for t in text:\n",
    "        split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "        tokens = model.to_tokens(t, prepend_bos=False)\n",
    "        original_logits = model(tokens)\n",
    "        ablated_logits = ablate_mlp_neurons(tokens, neurons)\n",
    "        diff_logits = original_logits - ablated_logits\n",
    "        if setting == \"true_tokens\":\n",
    "            # Gather the logits for the true tokens\n",
    "            diff = diff_logits.gather(2,tokens.unsqueeze(2)).squeeze()\n",
    "        elif setting == \"max\":\n",
    "            diff = diff_logits.argmax(2)\n",
    "        text_list += split_text + [\"\\n\"]\n",
    "        logit_list += diff.tolist() + [0.0]\n",
    "    logit_list = th.tensor(logit_list).reshape(-1,1,1)\n",
    "    print(f\"Max & Min logit-diff: {logit_list.max().item():.2f} & {logit_list.min().item():.2f}\")\n",
    "    return text_neuron_activations(tokens=text_list, activations=logit_list)\n",
    "# def visualize_logit_diff(tokens, neurons: th.Tensor, setting=\"true_tokens\"):\n",
    "setting = \"true_tokens\"\n",
    "tokens = model.to_tokens(\"testing 1 2 3\", prepend_bos=False)\n",
    "\n",
    "neurons = th.tensor([[924],[1306]])\n",
    "# neurons = th.tensor([[0]])\n",
    "\n",
    "text = [\n",
    "    \"testing 1 2 3\",\n",
    "    \" a b c\",\n",
    "    \" d e f\",\n",
    "    \" g h\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['testing', ' 1', ' 2', ' 3'],\n",
       " [' a', ' b', ' c'],\n",
       " [' d', ' e', ' f'],\n",
       " [' g', ' h'],\n",
       " '\\n',\n",
       " ['testing', ' 1', ' 2', ' 3'],\n",
       " [' a', ' b', ' c'],\n",
       " [' d', ' e', ' f'],\n",
       " [' g', ' h'],\n",
       " '\\n',\n",
       " ['testing', ' 1', ' 2', ' 3'],\n",
       " [' a', ' b', ' c'],\n",
       " [' d', ' e', ' f'],\n",
       " [' g', ' h'],\n",
       " '\\n',\n",
       " ['testing', ' 1', ' 2', ' 3'],\n",
       " [' a', ' b', ' c'],\n",
       " [' d', ' e', ' f'],\n",
       " [' g', ' h'],\n",
       " '\\n']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-88ba7e29-48f0\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, ColoredTokens } from \"https://unpkg.com/circuitsvis@1.39.1/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-88ba7e29-48f0\",\n",
       "      ColoredTokens,\n",
       "      {\"tokens\": [\"hey\"], \"values\": [1.3]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f9e785845e0>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from circuitsvis.tokens import colored_tokens\n",
    "colored_tokens([\"hey\"], [1.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.3670,  5.0511,  7.9925, 12.5219], device='cuda:2')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_logits.gather(2,tokens[:,1:].unsqueeze(2)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 19462,   337,   374,   495]], device='cuda:2')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the value tensor: torch.Size([1, 14, 2048])\n",
      "Shape of the value tensor: torch.Size([1, 14, 2048])\n"
     ]
    }
   ],
   "source": [
    "# I want to clamp the activation of the neuron to, say 0.\n",
    "\n",
    "neurons = th.tensor([[924],[1306]])\n",
    "text = \"Q:\\n\\n What is the capital of France?\\n\\n Paris.\"\n",
    "tokens = model.to_tokens(text)\n",
    "original_logits, cache = model.run_with_cache(tokens)\n",
    "original_neuron_activation = cache[\"blocks.1.mlp.hook_post\"].to(device)\n",
    "# def mlp_ablation_hook(\n",
    "#     value: Float[th.Tensor, \"batch pos d_mlp\"],\n",
    "#     hook: HookPoint\n",
    "# ) -> Float[th.Tensor, \"batch pos d_mlp\"]:\n",
    "#     print(f\"Shape of the value tensor: {value.shape}\")\n",
    "#     value[:, :, neurons] = 0.\n",
    "#     return value\n",
    "text = \" 1 2 3 4 5 6 7 8 9 10 11 12 13\"\n",
    "tokens = model.to_tokens(text)\n",
    "patching_logits, cache = model.run_with_cache(tokens)\n",
    "patching_neuron_activation = cache[\"blocks.0.mlp.hook_post\"].to(device)\n",
    "\n",
    "\n",
    "\n",
    "def mlp_ablation_hook(\n",
    "    value: Float[th.Tensor, \"batch pos d_mlp\"],\n",
    "    hook: HookPoint\n",
    ") -> Float[th.Tensor, \"batch pos d_mlp\"]:\n",
    "    print(f\"Shape of the value tensor: {value.shape}\")\n",
    "    value[:, :, :] = patching_neuron_activation[:, :, :]\n",
    "    return value\n",
    "\n",
    "def mlp_new_neuron_hook(\n",
    "    value: Float[th.Tensor, \"batch pos d_mlp\"],\n",
    "    hook: HookPoint\n",
    ") -> Float[th.Tensor, \"batch pos d_mlp\"]:\n",
    "    print(f\"Shape of the value tensor: {value.shape}\")\n",
    "    value[:, :, neurons[0]] = original_neuron_activation[:, :, neurons[0]]\n",
    "    value[:, :, neurons[1]] = original_neuron_activation[:, :, neurons[1]]\n",
    "    return value\n",
    "\n",
    "new_logits = model.run_with_hooks(tokens, fwd_hooks=[(\"blocks.0.mlp.hook_post\", mlp_ablation_hook), \n",
    "                                                     (f\"blocks.{layer}.mlp.hook_post\", mlp_new_neuron_hook)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0388, -0.1204, -0.0145,  ..., -0.0833,  0.0398, -0.1080],\n",
       "         [-0.0575, -0.1016, -0.1475,  ..., -0.1400, -0.1365,  0.3035],\n",
       "         [-0.1224, -0.1228, -0.1077,  ..., -0.1422, -0.1699, -0.1699],\n",
       "         ...,\n",
       "         [-0.1521, -0.1035, -0.1437,  ..., -0.1474, -0.1442, -0.1580],\n",
       "         [-0.1348, -0.1215, -0.1167,  ..., -0.1240, -0.1696,  0.1118],\n",
       "         [-0.1469, -0.1357, -0.0492,  ..., -0.1335, -0.0162, -0.1569]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_neuron_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<|endoftext|>' 0.0\n",
      "'5' 10.500371932983398\n",
      "'nd' 10.349782943725586\n",
      "')].' 13.748017311096191\n",
      "'ти' 14.573488235473633\n",
      "'yrs' 14.681735038757324\n",
      "'idemargin' 14.050003051757812\n",
      "'ificance' 18.274391174316406\n",
      "']).' 18.546470642089844\n",
      "'))$.' 16.930084228515625\n",
      "')\\\\].' 15.339709281921387\n",
      "' 12' 11.824020385742188\n",
      "'tml' 13.262931823730469\n",
      "')\\\\].' 17.10406494140625\n",
      "======================\n",
      "\n",
      "'\\n' 11.203080177307129\n",
      "':' 12.916642189025879\n",
      "'\\n' 17.881086349487305\n",
      "'\\n' 16.53376579284668\n",
      "' is' 15.666515350341797\n",
      "' the' 16.361787796020508\n",
      "' difference' 14.399608612060547\n",
      "' of' 16.922134399414062\n",
      "' the' 15.69821548461914\n",
      "'?' 19.01654052734375\n",
      "'\\n' 19.311105728149414\n",
      "'\\n' 16.671977996826172\n",
      "' is' 15.80677604675293\n",
      "'\\n' 16.598018646240234\n"
     ]
    }
   ],
   "source": [
    "# Find the largest difference for each token\n",
    "diff = new_logits- original_logits\n",
    "diff = diff.abs()\n",
    "diff_val, diff_ind = diff.max(dim=-1)\n",
    "out = model.to_str_tokens(diff_ind)\n",
    "for o, v in zip(out, diff_val[0]):\n",
    "    print(repr(o), v.item())\n",
    "print(\"======================\\n\")\n",
    "val,ind = original_logits.max(dim=-1)\n",
    "out = model.to_str_tokens(ind)\n",
    "for o, v in zip(out, val[0]):\n",
    "    print(repr(o), v.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\n' 11.203080177307129\n",
      "'.' 13.492734909057617\n",
      "' 3' 14.281828880310059\n",
      "' 4' 17.700979232788086\n",
      "' 5' 17.17980194091797\n",
      "' 3' 17.17455291748047\n",
      "' 6' 17.38115882873535\n",
      "' 8' 19.010574340820312\n",
      "' 8' 19.93307876586914\n",
      "' 9' 19.177921295166016\n",
      "' 7' 17.89773178100586\n",
      "' 11' 18.138347625732422\n",
      "' 12' 17.840625762939453\n",
      "' 12' 17.692947387695312\n"
     ]
    }
   ],
   "source": [
    "val,ind = new_logits.max(dim=-1)\n",
    "out = model.to_str_tokens(ind)\n",
    "for o, v in zip(out, val[0]):\n",
    "    print(repr(o), v.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " is\n",
      " the\n",
      " difference\n",
      " of\n",
      " the\n",
      "?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " is\n",
      "\n",
      "\n",
      "tensor([11.2031, 12.9166, 17.8811, 16.5338, 15.6665, 16.3618, 14.3996, 16.9221,\n",
      "        15.6982, 19.0165, 19.3111, 16.6720, 15.8068, 16.5980], device='cuda:2',\n",
      "       grad_fn=<UnbindBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for o in out:\n",
    "    print(o)\n",
    "for v in val:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-78420411-ca9c\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, ColoredTokens } from \"https://unpkg.com/circuitsvis@1.39.1/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-78420411-ca9c\",\n",
       "      ColoredTokens,\n",
       "      {\"tokens\": [\"hey\"], \"values\": [1.3]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f9e78584d60>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from circuitsvis.tokens import colored_tokens\n",
    "colored_tokens([\"hey\"], [1.3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
