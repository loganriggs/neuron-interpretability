{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-160m-deduped into HookedTransformer\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Import Transformer Lens, and load pythia models\n",
    "from transformer_lens import HookedTransformer\n",
    "import torch as th\n",
    "from torch import nn\n",
    "device = \"cuda\" if th.cuda.is_available() else \"cpu\"\n",
    "model = HookedTransformer.from_pretrained(\"EleutherAI/pythia-160m-deduped\", device=device)\n",
    "class Neuron_Max(nn.Module):\n",
    "    def __init__(self, model: nn.Module, layer: int):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.model.requires_grad_(False)\n",
    "        self.embed_weights = list(list(model.children())[0].parameters())[0]\n",
    "        transformer_blocks = [mod for mod in list(self.model.children())[2]]\n",
    "        self.model_no_embed = th.nn.Sequential(*(transformer_blocks[:layer+1])).requires_grad_(False)\n",
    "        self.model_no_embed.requires_grad_(False)\n",
    "        self._neurons = th.empty(0)\n",
    "        def hook(model, input, output):\n",
    "            self._neurons = output\n",
    "        self.model.blocks[layer].mlp.hook_pre.register_forward_hook(hook)\n",
    "        # self.model.blocks[layer].hook_mlp_out.register_forward_hook(hook)\n",
    "\n",
    "        \n",
    "    def embedded_forward(self, embedded_x):\n",
    "        self.model_no_embed(embedded_x)\n",
    "        return self._neurons\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.model(x)       \n",
    "        return self._neurons\n",
    "    \n",
    "    def run_with_cache(self, x):\n",
    "        return self.model.run_with_cache(x, remove_batch_dim=True)\n",
    "    \n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "tokens = model.to_tokens(text)\n",
    "embedded_tokens = th.nn.Parameter(model.embed(tokens))\n",
    "embedded_tokens.requires_grad = True\n",
    "layer = 6\n",
    "neuron = 3069\n",
    "\n",
    "hook_model = Neuron_Max(model, layer)\n",
    "mlp_pre = hook_model.embedded_forward(embedded_tokens)\n",
    "\n",
    "_, cache = model.run_with_cache(tokens, remove_batch_dim=False)\n",
    "# mlp_pre_original = cache[f\"blocks.{layer}.hook_mlp_out\"]\n",
    "mlp_pre_original = cache[f\"blocks.{layer}.mlp.hook_pre\"]\n",
    "# Make sure both are equal\n",
    "print(th.allclose(mlp_pre, mlp_pre_original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting diverse output 0\n",
      "New largest activation: 0.04066186770796776 | [' 1 2 3 4']\n",
      "New largest activation: 0.9042648673057556 | ['ahan accuracy 2009  ']\n",
      "New largest activation: 1.0830827951431274 | [' terrific fuss 83 737']\n",
      "New largest activation: 1.1185054779052734 | ['ako tod 07 737']\n",
      "New largest activation: 1.2184388637542725 | ['kéKH 07 balcon']\n",
      "New largest activation: 1.2394092082977295 | ['kéKH minute balcon']\n",
      "New largest activation: 1.3135446310043335 | [' osKHzech nä']\n",
      "New largest activation: 1.3617918491363525 | [' któcad h balcon']\n",
      "New largest activation: 1.3642041683197021 | ['okratNos h balcon']\n",
      "New largest activation: 1.4049625396728516 | ['ímNos dé 737']\n",
      "New largest activation: 1.4831815958023071 | [' wordt viief Fl']\n",
      "New largest activation: 1.6548222303390503 | [' Cott ti že flaws']\n",
      "New largest activation: 1.6892642974853516 | [' že vi dib balcon']\n",
      "New largest activation: 1.7998237609863281 | ['atie tem dib Fl']\n",
      "New largest activation: 2.021145820617676 | [' kaois č flaws']\n",
      "New largest activation: 2.1016650199890137 | ['tetra bere č flaws']\n",
      "New largest activation: 2.185164213180542 | [' ka kaýmстав']\n",
      "Starting diverse output 1\n",
      "New largest activation: 0.04066186770796776 | [' 1 2 3 4']\n",
      "New largest activation: 1.1638009548187256 | ['icken 1994 2009  ']\n",
      "New largest activation: 1.2308146953582764 | ['ças dubbed enqu blown']\n",
      "New largest activation: 1.6051312685012817 | [' tutticlinical bere blown']\n",
      "New largest activation: 1.7626959085464478 | ['ésidentzent qu Brun']\n",
      "New largest activation: 2.1132664680480957 | [' detainikh săcred']\n",
      "Starting diverse output 2\n",
      "New largest activation: 0.04066186770796776 | [' 1 2 3 4']\n",
      "New largest activation: 1.1938254833221436 | ['ahanpone 2009  ']\n",
      "New largest activation: 1.6927695274353027 | [' terrific fuss 83 MON']\n",
      "New largest activation: 2.050387382507324 | [' ça eyバ balcon']\n",
      "New largest activation: 2.0685527324676514 | [' cornealiejHocred']\n",
      "New largest activation: 2.0873472690582275 | ['ésident TakHocred']\n",
      "Starting diverse output 3\n",
      "New largest activation: 0.04066186770796776 | [' 1 2 3 4']\n",
      "New largest activation: 1.1938254833221436 | ['ahanpone 2009  ']\n",
      "New largest activation: 1.3648592233657837 | [' toujours Today enqu balcon']\n",
      "New largest activation: 1.7459194660186768 | [' softer Today biod liqu']\n",
      "New largest activation: 1.7629857063293457 | ['zech meningJecred']\n",
      "New largest activation: 1.7742570638656616 | ['zech meningJe balcon']\n",
      "New largest activation: 1.9845917224884033 | [' ist Adri č flaws']\n",
      "Starting diverse output 4\n",
      "New largest activation: 0.04066186770796776 | [' 1 2 3 4']\n",
      "New largest activation: 1.1938254833221436 | ['ahanpone 2009  ']\n",
      "New largest activation: 1.7307801246643066 | ['icken commentary quand cin']\n",
      "New largest activation: 1.8137452602386475 | [' rugby ça Fl balcon']\n",
      "New largest activation: 2.0098276138305664 | [' política ça Fl balcon']\n",
      "Starting diverse output 5\n",
      "New largest activation: 0.04066186770796776 | [' 1 2 3 4']\n",
      "New largest activation: 1.1938254833221436 | ['ahanpone 2009  ']\n",
      "New largest activation: 1.2352172136306763 | [' excellentAldrich 95 morning']\n",
      "New largest activation: 1.2705570459365845 | [' excellent sevent 58 morning']\n",
      "New largest activation: 1.5006557703018188 | [' notably sevent 57 morning']\n",
      "New largest activation: 1.5499567985534668 | [' ou Adri 57 MON']\n",
      "New largest activation: 1.6519689559936523 | ['ière Sund 47 morning']\n",
      "New largest activation: 1.8616454601287842 | ['ière któ pel balcon']\n",
      "New largest activation: 1.951507329940796 | ['atieť fe blown']\n",
      "Starting diverse output 6\n",
      "New largest activation: 0.04066186770796776 | [' 1 2 3 4']\n",
      "New largest activation: 1.1938254833221436 | ['ahanpone 2009  ']\n",
      "New largest activation: 1.2534410953521729 | [' softersudonine flaws']\n",
      "New largest activation: 1.2772178649902344 | [' tempo regards539 balcon']\n",
      "New largest activation: 1.394655466079712 | [' muito tasty539 beaut']\n",
      "New largest activation: 1.5921608209609985 | [' seafood boasts467 MON']\n",
      "New largest activation: 1.6698766946792603 | ['iotics să907 flaws']\n",
      "New largest activation: 1.6804299354553223 | ['atie sămo flaws']\n",
      "New largest activation: 1.7030715942382812 | ['tera č 94 redund']\n",
      "New largest activation: 1.856374979019165 | [' Vik č763 redund']\n",
      "Starting diverse output 7\n",
      "New largest activation: 0.04066186770796776 | [' 1 2 3 4']\n",
      "New largest activation: 0.9042648673057556 | ['ahan accuracy 2009  ']\n",
      "New largest activation: 1.0567214488983154 | [' terrific economical 83Looks']\n",
      "New largest activation: 1.142793893814087 | [' touteubottu)-\\\\ HTTP']\n",
      "New largest activation: 1.1438865661621094 | [' todos\\x9bmolecules balcon']\n",
      "New largest activation: 1.2393994331359863 | [' todoskubuntu filmmaker balcon']\n",
      "New largest activation: 1.3804224729537964 | [' todosango filmmaker balcon']\n",
      "New largest activation: 1.402506709098816 | [' avait fuss ey 737']\n",
      "New largest activation: 1.4248006343841553 | [' nos proble ey balcon']\n",
      "New largest activation: 1.44207763671875 | [' és kad)-\\\\ MON']\n",
      "New largest activation: 1.5398166179656982 | [' kad kad dé migraine']\n",
      "New largest activation: 1.7137680053710938 | [' ésisty kaстав']\n",
      "New largest activation: 2.1738510131835938 | [' kadisty čстав']\n",
      "Starting diverse output 8\n",
      "New largest activation: 0.04066186770796776 | [' 1 2 3 4']\n",
      "New largest activation: 1.1489661931991577 | ['ahan499 2009  ']\n",
      "New largest activation: 1.4044058322906494 | [' terrific fuss J Fl']\n",
      "New largest activation: 1.5545578002929688 | [' polit fuss J MON']\n",
      "New largest activation: 1.6992809772491455 | [' nacional pH č reper']\n",
      "New largest activation: 1.737379550933838 | ['ava 94 č reper']\n",
      "New largest activation: 1.769421100616455 | ['ität Aj ž Fl']\n",
      "New largest activation: 1.9229021072387695 | [' čgos dib feas']\n",
      "New largest activation: 1.9745830297470093 | ['néjp tem blown']\n",
      "Starting diverse output 9\n",
      "New largest activation: 0.04066186770796776 | [' 1 2 3 4']\n",
      "New largest activation: 1.1489661931991577 | ['ahan499 2009  ']\n",
      "New largest activation: 1.169464349746704 | [' terrific pH J718']\n",
      "New largest activation: 1.5114655494689941 | [' хighed wonder reper']\n",
      "New largest activation: 1.7353968620300293 | ['ké alimentvé balcon']\n",
      "New largest activation: 1.7638521194458008 | [' Cottika č feas']\n",
      "New largest activation: 1.7793667316436768 | [' Cott Comissão č feas']\n",
      "New largest activation: 2.004460096359253 | ['áv briefing čстав']\n",
      "New largest activation: 2.046062707901001 | [' Cott mening čстав']\n"
     ]
    }
   ],
   "source": [
    "# for neuron in range(100):\n",
    "\n",
    "# init_text = \"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\"\n",
    "# init_text = \" 1 a ; d\"\n",
    "# init_text = ''' Pavel Rovinski\n",
    "# neuron = 0\n",
    "\n",
    "# Pavel Apolonovič Rovinski (1831'''\n",
    "#TODO make work w/ 1 token\n",
    "layer=6\n",
    "# neuron = 493\n",
    "epochs =200\n",
    "\n",
    "diverse_outputs_num = 10\n",
    "_, _, embed_size = model.W_out.shape\n",
    "seq = 4 #TODO make this work more functionally\n",
    "keep_last_token = True\n",
    "diverse_outputs = th.zeros(diverse_outputs_num, seq, embed_size)\n",
    "largest_prompts = [None]*diverse_outputs_num\n",
    "cos = th.nn.CosineSimilarity(dim=1)\n",
    "for d_ind in range(diverse_outputs_num):\n",
    "    print(f\"Starting diverse output {d_ind}\")\n",
    "    # init_text = \" the injuries England have\"\n",
    "    init_text = \" 1 2 3 4\"\n",
    "    init_tokens = model.to_tokens(init_text, prepend_bos=False)\n",
    "    prompt_embeds = th.nn.Parameter(model.embed(init_tokens)).detach()\n",
    "    prompt_embeds.requires_grad_(True)\n",
    "\n",
    "    optim = th.optim.AdamW([prompt_embeds], lr=.8, weight_decay=0.01)\n",
    "    largest_activation = 0\n",
    "    largest_prompt = None\n",
    "    for i in range(epochs):\n",
    "        # First, project into the embedding matrix\n",
    "        with th.no_grad():\n",
    "            projected_index = th.stack([(hook_model.embed_weights@prompt_embeds[0,i,:]).argmax() for i in range(seq)]).unsqueeze(0)\n",
    "            projected_embeds = model.embed(projected_index)\n",
    "\n",
    "        # Create a temp embedding that is detached from the graph, but has the same data as the projected embedding\n",
    "        tmp_embeds = prompt_embeds.detach().clone()\n",
    "        tmp_embeds.data = projected_embeds.data\n",
    "        # add some gaussian noise to tmp_embeds\n",
    "        # tmp_embeds.data += th.randn_like(tmp_embeds.data)*0.01\n",
    "        tmp_embeds.requires_grad_(True)\n",
    "\n",
    "\n",
    "        # Then, calculate neuron_output\n",
    "        neuron_output = hook_model.embedded_forward(tmp_embeds)[0,:, neuron]\n",
    "        diversity_loss = cos(tmp_embeds[0], diverse_outputs[:d_ind]) #TODO, check if this is correct\n",
    "        loss = -neuron_output[-1] + diversity_loss.mean()\n",
    "\n",
    "        # Save the highest activation\n",
    "        if neuron_output[-1] > largest_activation:\n",
    "            largest_activation = neuron_output[-1]\n",
    "            largest_prompt = model.to_string(projected_index)\n",
    "            largest_prompts[d_ind] = largest_prompt\n",
    "            print(f\"New largest activation: {largest_activation} | {largest_prompt}\")\n",
    "\n",
    "        # Transfer the gradient to the continuous embedding space\n",
    "        prompt_embeds.grad, = th.autograd.grad(loss, [tmp_embeds])\n",
    "        \n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "    diverse_outputs[d_ind] = tmp_embeds.data[0,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting diverse output 0\n",
      "New largest activation: 1.8464672565460205 | [' 1 2 3 p.m']\n",
      "New largest activation: 1.9286843538284302 | [' Companies\\t\\t\\t\\t\\t\\t\\t\\t dusk p.m']\n",
      "New largest activation: 2.2665884494781494 | [' CompaniesOOGLEinety p.m']\n",
      "New largest activation: 2.324343681335449 | [' formations aboard decade p.m']\n",
      "New largest activation: 2.385915756225586 | [' evacuation flaresinety p.m']\n",
      "New largest activation: 2.3997743129730225 | [' evacuation kilomet Forty p.m']\n",
      "New largest activation: 2.439774751663208 | [' aircraft kilomet 2030 p.m']\n",
      "New largest activation: 2.6292884349823 | [' march kilomet 900 p.m']\n",
      "New largest activation: 2.658616542816162 | [' march kilomet 800 p.m']\n",
      "New largest activation: 2.7313034534454346 | [' trainsieurs eighty p.m']\n",
      "New largest activation: 2.745493173599243 | [' hikingieurs eighty p.m']\n",
      "New largest activation: 2.7665293216705322 | [' hikeieurs eighty p.m']\n",
      "New largest activation: 2.912780523300171 | [' hike\\n \\n096 p.m']\n",
      "Starting diverse output 1\n",
      "New largest activation: 1.8464672565460205 | [' 1 2 3 p.m']\n",
      "New largest activation: 2.156503438949585 | [' FOUR evacuation dusk p.m']\n",
      "New largest activation: 2.20666241645813 | [' Pictures evacuation kilomet p.m']\n",
      "New largest activation: 2.4611494541168213 | [' sculpture August kilomet p.m']\n",
      "New largest activation: 2.4875004291534424 | [' theatre flaresinety p.m']\n",
      "New largest activation: 2.5052154064178467 | [' Socorroquarters 105 p.m']\n",
      "New largest activation: 2.643918752670288 | ['deen creekinety p.m']\n",
      "New largest activation: 2.7259552478790283 | [' evenings creekinety p.m']\n",
      "New largest activation: 2.7916316986083984 | [' sculpture creek38 p.m']\n",
      "Starting diverse output 2\n",
      "New largest activation: 1.8464672565460205 | [' 1 2 3 p.m']\n",
      "New largest activation: 2.073413133621216 | [' ophthal surroundings river p.m']\n",
      "New largest activation: 2.0817553997039795 | [' ophthal evacuation river p.m']\n",
      "New largest activation: 2.1356122493743896 | [' osteoclast evacuation river p.m']\n",
      "New largest activation: 2.1770176887512207 | [' osteoclast 250 1915 p.m']\n",
      "New largest activation: 2.1794180870056152 | [' June meters 1915 p.m']\n",
      "New largest activation: 2.202331066131592 | [' Adventures meters1934 p.m']\n",
      "New largest activation: 2.6221225261688232 | [' November kilomet meters p.m']\n",
      "New largest activation: 2.6282737255096436 | [' vegetationopters kilomet p.m']\n",
      "Starting diverse output 3\n",
      "New largest activation: 1.8464672565460205 | [' 1 2 3 p.m']\n",
      "New largest activation: 2.07304310798645 | [' Aurora evacuation dusk p.m']\n",
      "New largest activation: 2.082388401031494 | [' August travelledπό p.m']\n",
      "New largest activation: 2.4024717807769775 | [' dawn evacuation kilomet p.m']\n",
      "New largest activation: 2.571904420852661 | [' dawn drifted kilomet p.m']\n",
      "New largest activation: 2.6809091567993164 | [' August whales kilomet p.m']\n",
      "New largest activation: 2.7439608573913574 | [' tribut drifted38 p.m']\n",
      "New largest activation: 2.937380075454712 | [' tribut drifted967 p.m']\n",
      "Starting diverse output 4\n",
      "New largest activation: 1.8464672565460205 | [' 1 2 3 p.m']\n",
      "New largest activation: 1.914067268371582 | ['egan municip evenings p.m']\n",
      "New largest activation: 2.26112961769104 | ['geois� kilomet p.m']\n",
      "New largest activation: 2.277597665786743 | [' begun\\t\\t\\t\\t\\t\\t\\t\\t meters p.m']\n",
      "New largest activation: 2.4919486045837402 | [' begun kil meters p.m']\n",
      "New largest activation: 2.554786205291748 | [' June\\t\\t\\t\\t\\t\\t\\t\\t kilomet p.m']\n",
      "New largest activation: 2.7028989791870117 | [' Hundred meters kilomet p.m']\n",
      "New largest activation: 2.79702091217041 | [' dawn Fifty kilomet p.m']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\logan\\AppData\\Local\\Temp\\ipykernel_14128\\1170155933.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">35</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\logan\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_14128\\\\1170155933.py'</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\logan\\AppData\\Local\\Temp\\ipykernel_14128\\1170155933.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">35</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;listcomp&gt;</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\logan\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_14128\\\\1170155933.py'</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\logan\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1194</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1191 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1192 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1193 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1194 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1195 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1196 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1197 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\logan\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\distance.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">87</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">84 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.eps = eps                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">85 │   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">86 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, x1: Tensor, x2: Tensor) -&gt; Tensor:                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>87 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> F.cosine_similarity(x1, x2, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dim, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.eps)                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">88 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\logan\\AppData\\Local\\Temp\\ipykernel_14128\\1170155933.py\u001b[0m:\u001b[94m35\u001b[0m in \u001b[92m<module>\u001b[0m                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m'C:\\\\Users\\\\logan\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_14128\\\\1170155933.py'\u001b[0m                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\logan\\AppData\\Local\\Temp\\ipykernel_14128\\1170155933.py\u001b[0m:\u001b[94m35\u001b[0m in \u001b[92m<listcomp>\u001b[0m                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m'C:\\\\Users\\\\logan\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_14128\\\\1170155933.py'\u001b[0m                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\logan\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m:\u001b[94m1194\u001b[0m in \u001b[92m_call_impl\u001b[0m        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1191 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1192 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1193 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1194 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1195 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1196 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1197 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\logan\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\distance.py\u001b[0m:\u001b[94m87\u001b[0m in \u001b[92mforward\u001b[0m           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m84 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.eps = eps                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m85 \u001b[0m\u001b[2m│   \u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m86 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mforward\u001b[0m(\u001b[96mself\u001b[0m, x1: Tensor, x2: Tensor) -> Tensor:                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m87 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m F.cosine_similarity(x1, x2, \u001b[96mself\u001b[0m.dim, \u001b[96mself\u001b[0m.eps)                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m88 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for neuron in range(100):\n",
    "\n",
    "# init_text = \"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\"\n",
    "# init_text = \" 1 a ; d\"\n",
    "# init_text = ''' Pavel Rovinski\n",
    "# neuron = 0\n",
    "\n",
    "# Pavel Apolonovič Rovinski (1831'''\n",
    "#TODO make work w/ 1 token\n",
    "layer=6\n",
    "# neuron = 493\n",
    "epochs =200\n",
    "\n",
    "diverse_outputs_num = 10\n",
    "_, _, embed_size = model.W_out.shape\n",
    "seq = 3 #TODO make this work more functionally\n",
    "insert_token = True\n",
    "diverse_outputs = th.zeros(diverse_outputs_num, seq, embed_size)\n",
    "largest_prompts = [None]*diverse_outputs_num\n",
    "cos = th.nn.CosineSimilarity(dim=1)\n",
    "for d_ind in range(diverse_outputs_num):\n",
    "    print(f\"Starting diverse output {d_ind}\")\n",
    "    # init_text = \" the injuries England have\"\n",
    "    init_text = \" 1 2 3\"\n",
    "    init_tokens = model.to_tokens(init_text, prepend_bos=False)\n",
    "    prompt_embeds = th.nn.Parameter(model.embed(init_tokens)).detach()\n",
    "    prompt_embeds.requires_grad_(True)\n",
    "\n",
    "    optim = th.optim.AdamW([prompt_embeds], lr=.8, weight_decay=0.01)\n",
    "    largest_activation = 0\n",
    "    largest_prompt = None\n",
    "\n",
    "    iterations_since_last_improvement = 0\n",
    "    while(iterations_since_last_improvement < 30):\n",
    "    # First, project into the embedding matrix\n",
    "        with th.no_grad():\n",
    "            projected_index = th.stack([cos(hook_model.embed_weights,prompt_embeds[0,i,:]).argmax() for i in range(seq)]).unsqueeze(0)\n",
    "            projected_embeds = model.embed(projected_index)\n",
    "\n",
    "        # Create a temp embedding that is detached from the graph, but has the same data as the projected embedding\n",
    "        tmp_embeds = prompt_embeds.detach().clone()\n",
    "        tmp_embeds.data = projected_embeds.data\n",
    "        # add some gaussian noise to tmp_embeds\n",
    "        # tmp_embeds.data += th.randn_like(tmp_embeds.data)*0.01\n",
    "        tmp_embeds.requires_grad_(True)\n",
    "\n",
    "        if insert_token:\n",
    "            text = \" p.m\"\n",
    "            token = model.to_tokens(text, prepend_bos=False)\n",
    "            token_embeds = model.embed(token)\n",
    "            token_pos = seq\n",
    "            wrapped_embeds = th.cat([tmp_embeds[0,:token_pos], token_embeds[0], tmp_embeds[0,token_pos:]], dim=0).unsqueeze(0)\n",
    "        else:\n",
    "            wrapped_embeds = tmp_embeds\n",
    "\n",
    "        # Then, calculate neuron_output\n",
    "        neuron_output = hook_model.embedded_forward(wrapped_embeds)[0,:, neuron]\n",
    "        diversity_loss = cos(tmp_embeds[0], diverse_outputs[:d_ind]) #TODO, check if this is correct\n",
    "        loss = -neuron_output[-1] + diversity_loss.mean()\n",
    "\n",
    "        # Save the highest activation\n",
    "        if neuron_output[-1] > largest_activation:\n",
    "            iterations_since_last_improvement = 0\n",
    "            largest_activation = neuron_output[-1]\n",
    "            wrapped_embeds_seq_len = wrapped_embeds.shape[1]\n",
    "            projected_index = th.stack([cos(hook_model.embed_weights,wrapped_embeds[0,i,:]).argmax() for i in range(wrapped_embeds_seq_len)]).unsqueeze(0)\n",
    "            largest_prompt = model.to_string(projected_index)\n",
    "            largest_prompts[d_ind] = largest_prompt\n",
    "            print(f\"New largest activation: {largest_activation} | {largest_prompt}\")\n",
    "\n",
    "        # Transfer the gradient to the continuous embedding space\n",
    "        prompt_embeds.grad, = th.autograd.grad(loss, [tmp_embeds])\n",
    "        \n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "    diverse_outputs[d_ind] = tmp_embeds.data[0,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer=6\n",
    "\n",
    "def prompt_optimization(\n",
    "        model, \n",
    "        neuron, \n",
    "        diverse_outputs_num=10, \n",
    "        iteration_cap_until_convergence = 30,\n",
    "        init_text = None,\n",
    "        seq_size = 4,\n",
    "        insert_words_and_pos = None, #List of words and positions to insert [word, pos]\n",
    "        neuron_loss_scalar = 1,\n",
    "        diversity_loss_scalar = 1,\n",
    "    ):\n",
    "    _, _, embed_size = model.W_out.shape\n",
    "    vocab_size = model.W_E.shape[0]\n",
    "    largest_prompts = [None]*diverse_outputs_num\n",
    "    cos = th.nn.CosineSimilarity(dim=1)\n",
    "    total_iterations = 0\n",
    "\n",
    "    if init_text is not None:\n",
    "        init_tokens = model.to_tokens(init_text, prepend_bos=False)\n",
    "        seq_size = init_tokens.shape[-1]\n",
    "    diverse_outputs = th.zeros(diverse_outputs_num, seq_size, embed_size)\n",
    "    for d_ind in range(diverse_outputs_num):\n",
    "        print(f\"Starting diverse output {d_ind}\")\n",
    "        if init_text is None:\n",
    "            # Random tokens of sequence length\n",
    "            init_tokens = th.randint(0, vocab_size, (1,seq_size))\n",
    "            init_text = model.to_string(init_tokens)\n",
    "        prompt_embeds = th.nn.Parameter(model.embed(init_tokens)).detach()\n",
    "        prompt_embeds.requires_grad_(True)\n",
    "\n",
    "        optim = th.optim.AdamW([prompt_embeds], lr=.8, weight_decay=0.01)\n",
    "        largest_activation = 0\n",
    "        largest_prompt = None\n",
    "\n",
    "        iterations_since_last_improvement = 0\n",
    "        while(iterations_since_last_improvement < iteration_cap_until_convergence):\n",
    "        # First, project into the embedding matrix\n",
    "            with th.no_grad():\n",
    "                projected_index = th.stack([cos(hook_model.embed_weights,prompt_embeds[0,i,:]).argmax() for i in range(seq_size)]).unsqueeze(0)\n",
    "                projected_embeds = model.embed(projected_index)\n",
    "\n",
    "            # Create a temp embedding that is detached from the graph, but has the same data as the projected embedding\n",
    "            tmp_embeds = prompt_embeds.detach().clone()\n",
    "            tmp_embeds.data = projected_embeds.data\n",
    "            # add some gaussian noise to tmp_embeds\n",
    "            # tmp_embeds.data += th.randn_like(tmp_embeds.data)*0.01\n",
    "            tmp_embeds.requires_grad_(True)\n",
    "\n",
    "            if insert_words_and_pos is not None:\n",
    "                text = insert_words_and_pos[0]\n",
    "                pos = insert_words_and_pos[1]\n",
    "                if(pos == -1):\n",
    "                    pos = seq_size\n",
    "                token = model.to_tokens(text, prepend_bos=False)\n",
    "                token_embeds = model.embed(token)\n",
    "                token_pos = pos\n",
    "                wrapped_embeds = th.cat([tmp_embeds[0,:token_pos], token_embeds[0], tmp_embeds[0,token_pos:]], dim=0).unsqueeze(0)\n",
    "                if(total_iterations == 0):\n",
    "                    wrapped_embeds_seq_len = wrapped_embeds.shape[1]\n",
    "                    projected_index = th.stack([cos(hook_model.embed_weights,wrapped_embeds[0,i,:]).argmax() for i in range(wrapped_embeds_seq_len)]).unsqueeze(0)\n",
    "                    print(f\"Inserting {text} at pos {pos}: {model.to_str_tokens(projected_index, prepend_bos=False)}\")\n",
    "            else:\n",
    "                wrapped_embeds = tmp_embeds\n",
    "\n",
    "            # Then, calculate neuron_output\n",
    "            neuron_output = hook_model.embedded_forward(wrapped_embeds)[0,:, neuron]\n",
    "            diversity_loss = cos(tmp_embeds[0], diverse_outputs[:d_ind])\n",
    "            loss = neuron_loss_scalar*-neuron_output[-1] + diversity_loss_scalar*diversity_loss.mean()\n",
    "\n",
    "            # Save the highest activation\n",
    "            if neuron_output[-1] > largest_activation:\n",
    "                iterations_since_last_improvement = 0\n",
    "                largest_activation = neuron_output[-1]\n",
    "                wrapped_embeds_seq_len = wrapped_embeds.shape[1]\n",
    "                projected_index = th.stack([cos(hook_model.embed_weights,wrapped_embeds[0,i,:]).argmax() for i in range(wrapped_embeds_seq_len)]).unsqueeze(0)\n",
    "                largest_prompt = model.to_string(projected_index)\n",
    "                largest_prompts[d_ind] = largest_prompt\n",
    "                print(f\"New largest activation: {largest_activation} | {largest_prompt}\")\n",
    "\n",
    "            # Transfer the gradient to the continuous embedding space\n",
    "            prompt_embeds.grad, = th.autograd.grad(loss, [tmp_embeds])\n",
    "            \n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            total_iterations += 1\n",
    "            iterations_since_last_improvement += 1\n",
    "        diverse_outputs[d_ind] = tmp_embeds.data[0,...]\n",
    "    return largest_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting diverse output 0\n",
      "Inserting half at pos 8: ['in', ' a', ' cake', '-', 'and', '-', 'a', '-', 'half']\n",
      "New largest activation: 0.879313051700592 | ['in a cake-and-a-half']\n",
      "New largest activation: 0.9168182611465454 | [' myös Dawn spraywashurelandid Nem Havhalf']\n",
      "New largest activation: 1.0480660200119019 | [' myös Dawn sprayots gh guerraAz penghalf']\n",
      "New largest activation: 1.0770515203475952 | [' myösoga sprayots gh guerraAz penghalf']\n",
      "New largest activation: 1.1037309169769287 | [' toimoga sprayrina gh guerraAz penghalf']\n",
      "New largest activation: 1.1693141460418701 | [' któioxid sprayrina gh guerraAz hemodhalf']\n",
      "New largest activation: 1.2085641622543335 | [' któioxidajurina gh guerraAz hemodhalf']\n",
      "New largest activation: 1.3823320865631104 | ['ī Leonio Josapo selectivitynk Hohalf']\n",
      "New largest activation: 1.6561024188995361 | [' tä Leon tightening tribapoavaonas behhalf']\n",
      "New largest activation: 1.67436683177948 | [' tä LeonGil tribapoavank behhalf']\n",
      "New largest activation: 2.0089240074157715 | ['ų NyOs salavaľ mu behhalf']\n",
      "Starting diverse output 1\n",
      "New largest activation: 0.879313051700592 | ['in a cake-and-a-half']\n",
      "New largest activation: 0.9535455107688904 | [' partitions Frontier sprayresolve Okandid sea Havhalf']\n",
      "New largest activation: 1.2144743204116821 | [' denote Frontier Pairresolve Okandid sea hemodhalf']\n",
      "Starting diverse output 2\n",
      "New largest activation: 0.879313051700592 | ['in a cake-and-a-half']\n",
      "New largest activation: 1.1459795236587524 | [' partitionsioxid screenija Jak resistivityoa penghalf']\n",
      "New largest activation: 1.2211285829544067 | ['#chus hitsligt Bris ning Sri hemodhalf']\n",
      "New largest activation: 1.3916655778884888 | [' Consider Wilder Sentisu Jak mai Sri hemodhalf']\n",
      "New largest activation: 1.562758445739746 | [' Consider Wilder 72oto Jak mai Sri hemodhalf']\n",
      "New largest activation: 1.5774582624435425 | [' Consider Wilder 72oto Jak maink hemodhalf']\n",
      "Starting diverse output 3\n",
      "New largest activation: 0.879313051700592 | ['in a cake-and-a-half']\n",
      "Starting diverse output 4\n",
      "New largest activation: 0.879313051700592 | ['in a cake-and-a-half']\n",
      "New largest activation: 0.8884055614471436 | [' following FrontierliSpotraf dispensing sea Enhhalf']\n",
      "New largest activation: 1.1528762578964233 | [' following FrontierliSpotraf dispensing sea Havhalf']\n",
      "Starting diverse output 5\n",
      "New largest activation: 0.879313051700592 | ['in a cake-and-a-half']\n",
      "Starting diverse output 6\n",
      "New largest activation: 0.879313051700592 | ['in a cake-and-a-half']\n",
      "New largest activation: 1.0523604154586792 | [' Softwareioxid pageijaivan crefö fachalf']\n",
      "New largest activation: 1.0551042556762695 | [' Softwareioxid 4konih vivo Sri hemhalf']\n",
      "New largest activation: 1.1170915365219116 | [' Softwareioxid 4kojäadan Sri hemhalf']\n",
      "New largest activation: 1.4241492748260498 | [' Softwarechus 4konihadannk hemhalf']\n",
      "New largest activation: 1.6941293478012085 | [' Softwarechus”),konihadannk fachalf']\n",
      "Starting diverse output 7\n",
      "New largest activation: 0.879313051700592 | ['in a cake-and-a-half']\n",
      "New largest activation: 1.0589020252227783 | [' Softwarechus hydraulicijaivan crefö fachalf']\n",
      "New largest activation: 1.3247071504592896 | [' Softwareioxid hydraulicskynihز Sri fachalf']\n",
      "New largest activation: 1.486759901046753 | [' 52 ammonia secondseti najadan Sri dahalf']\n",
      "New largest activation: 1.7685956954956055 | [' 52 ammonia secondseti kaadan ih dahalf']\n",
      "Starting diverse output 8\n",
      "New largest activation: 0.879313051700592 | ['in a cake-and-a-half']\n",
      "Starting diverse output 9\n",
      "New largest activation: 0.879313051700592 | ['in a cake-and-a-half']\n",
      "New largest activation: 0.977069616317749 | [' stellar Frontier hydraulicija Shore resistivityoa danhalf']\n",
      "New largest activation: 1.1475898027420044 | ['ust Frontier hydraulicoto Panc guerraö�half']\n",
      "New largest activation: 1.203942060470581 | ['ust Fightingovyotoυτ guerraEU�half']\n",
      "New largest activation: 1.270668387413025 | ['ustск 52udi sedan guerra ni rhhalf']\n",
      "New largest activation: 1.5360229015350342 | [' gestскostoammadanni guerraaan dahalf']\n",
      "New largest activation: 1.6213617324829102 | [' gestскosto Antonanni guerraaan dahalf']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['ų NyOs salavaľ mu behhalf'],\n",
       " [' denote Frontier Pairresolve Okandid sea hemodhalf'],\n",
       " [' Consider Wilder 72oto Jak maink hemodhalf'],\n",
       " ['in a cake-and-a-half'],\n",
       " [' following FrontierliSpotraf dispensing sea Havhalf'],\n",
       " ['in a cake-and-a-half'],\n",
       " [' Softwarechus”),konihadannk fachalf'],\n",
       " [' 52 ammonia secondseti kaadan ih dahalf'],\n",
       " ['in a cake-and-a-half'],\n",
       " [' gestскosto Antonanni guerraaan dahalf']]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron = 3069\n",
    "prompt_optimization(\n",
    "    model, \n",
    "    neuron=neuron, \n",
    "    diverse_outputs_num=10, \n",
    "    iteration_cap_until_convergence = 10, \n",
    "    init_text = \"in a cake-and-a-\", \n",
    "    insert_words_and_pos = [\"half\", -1], \n",
    "    neuron_loss_scalar = 1, \n",
    "    diversity_loss_scalar = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_tokens(\"hey there\", prepend_bos=False).shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5139])\n",
      " AND\n"
     ]
    }
   ],
   "source": [
    "w1 = \" and\"\n",
    "w2 = \" AND\"\n",
    "e1 = model.embed(model.to_tokens(w1, prepend_bos=False))[0]\n",
    "e2 = model.embed(model.to_tokens(w2, prepend_bos=False))[0]\n",
    "cos_sim = cos(e1, e2)\n",
    "print(cos_sim)\n",
    "closest_unembed = cos(hook_model.embed_weights,e2[0,:]).argmax()\n",
    "print(model.to_string(closest_unembed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([0, 768])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_embeds[0,4:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting diverse output 0\n",
      "New largest activation: 0.7703665494918823 | [' pal/,compass Plymouth']\n",
      "New largest activation: 1.6871099472045898 | [' pal/,things Graham']\n",
      "New largest activation: 1.7203842401504517 | [' pal??Things moss']\n",
      "New largest activation: 2.0110278129577637 | [' pal.?Things moss']\n",
      "New largest activation: 2.4123072624206543 | [' synthes////////////////Things Payne']\n",
      "New largest activation: 2.856362819671631 | [' synthes\\xa0\\xa0things Hollywood']\n",
      "New largest activation: 3.1373682022094727 | [' synthes\\xa0\\xa0things Payne']\n",
      "New largest activation: 3.415623188018799 | [' Marcus.?things Clark']\n",
      "New largest activation: 3.474719524383545 | [' Marcus.?things Clark']\n",
      "New largest activation: 3.5846543312072754 | [' approach.?things Clark']\n",
      "Starting diverse output 1\n",
      "New largest activation: 3.281342029571533 | [' approach.?things Clark']\n",
      "New largest activation: 3.6527199745178223 | [' What kits Gonzalez Hogan']\n",
      "Starting diverse output 2\n",
      "New largest activation: 4.141073226928711 | [' What kits Gonzalez Hogan']\n",
      "Starting diverse output 3\n",
      "New largest activation: 3.8094820976257324 | [' What kits Gonzalez Hogan']\n",
      "Starting diverse output 4\n",
      "New largest activation: 3.180586814880371 | [' What kits Gonzalez Hogan']\n",
      "New largest activation: 3.580054759979248 | [' conclusions legislators Englishhave']\n",
      "New largest activation: 4.054036617279053 | [' bullshit libraries CWhave']\n",
      "New largest activation: 4.362361907958984 | [' anything programs Gonzalez underwent']\n",
      "New largest activation: 4.465662956237793 | [' anything programs Gonzalez underwent']\n",
      "New largest activation: 4.726469039916992 | [' comprom libraries Gonzalez Have']\n",
      "New largest activation: 4.7962212562561035 | [' comprom games Flash imposes']\n",
      "New largest activation: 5.196850776672363 | [' anecd treaties Gonzalez possesses']\n",
      "New largest activation: 5.302365303039551 | [' comprom percentages Lucy makes']\n",
      "New largest activation: 5.590920448303223 | [' treasures decisions Lucy wants']\n",
      "New largest activation: 5.740521430969238 | [' dozens tariffs Walmart took']\n",
      "New largest activation: 5.771655082702637 | [' analogous packages Weber makes']\n",
      "New largest activation: 5.808184623718262 | [' comprom packages Weber makes']\n",
      "New largest activation: 6.013857841491699 | [' dozens concepts Lucy wants']\n",
      "Starting diverse output 5\n",
      "New largest activation: 5.929004669189453 | [' dozens concepts Lucy wants']\n",
      "Starting diverse output 6\n",
      "New largest activation: 5.901174545288086 | [' dozens concepts Lucy wants']\n",
      "Starting diverse output 7\n",
      "New largest activation: 5.996420383453369 | [' dozens concepts Lucy wants']\n",
      "Starting diverse output 8\n",
      "New largest activation: 6.033523082733154 | [' dozens concepts Lucy wants']\n",
      "New largest activation: 6.0542449951171875 | [' needingthings Marcus makes']\n",
      "New largest activation: 6.2124457359313965 | [' comprom packages Lawson gave']\n",
      "New largest activation: 6.253775596618652 | [' WHAT treasures Lawson kept']\n",
      "Starting diverse output 9\n",
      "New largest activation: 5.981122970581055 | [' WHAT treasures Lawson kept']\n"
     ]
    }
   ],
   "source": [
    "# for neuron in range(100):\n",
    "\n",
    "# init_text = \"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\"\n",
    "# init_text = \" 1 a ; d\"\n",
    "# init_text = ''' Pavel Rovinski\n",
    "# neuron = 0\n",
    "\n",
    "# Pavel Apolonovič Rovinski (1831'''\n",
    "#TODO make work w/ 1 token\n",
    "iterative_initialization = True\n",
    "layer=6\n",
    "neuron = 493\n",
    "# neuron = 492\n",
    "epochs = 200\n",
    "diverse_outputs_num = 10\n",
    "_, _, embed_size = model.W_out.shape\n",
    "seq = 4 #TODO make this work more functionally\n",
    "diverse_outputs = th.zeros(diverse_outputs_num, seq, embed_size)\n",
    "largest_prompts = [None]*diverse_outputs_num\n",
    "cos = th.nn.CosineSimilarity(dim=1)\n",
    "for d_ind in range(diverse_outputs_num):\n",
    "    print(f\"Starting diverse output {d_ind}\")\n",
    "    # init_text = \" the injuries England have\"\n",
    "    # Random token\n",
    "    if(d_ind == 0 or not iterative_initialization):\n",
    "        init_tokens = th.randint(0, model.W_E.shape[0], (1, seq))\n",
    "    else:\n",
    "        init_tokens = largest_prompts[d_ind-1]\n",
    "    prompt_embeds = th.nn.Parameter(model.embed(init_tokens)).detach()\n",
    "    prompt_embeds.requires_grad_(True)\n",
    "\n",
    "    optim = th.optim.AdamW([prompt_embeds], lr=.5, weight_decay=0.01)\n",
    "    largest_activation = 0\n",
    "    largest_prompt = None\n",
    "\n",
    "    iterations_since_last_improvement = 0\n",
    "    while(iterations_since_last_improvement < 30):\n",
    "        # First, project into the embedding matrix\n",
    "        with th.no_grad():\n",
    "            projected_index = th.stack([(hook_model.embed_weights@prompt_embeds[0,i,:]).argmax() for i in range(seq)]).unsqueeze(0)\n",
    "            projected_embeds = model.embed(projected_index)\n",
    "\n",
    "        # Create a temp embedding that is detached from the graph, but has the same data as the projected embedding\n",
    "        tmp_embeds = prompt_embeds.detach().clone()\n",
    "        tmp_embeds.data = projected_embeds.data\n",
    "        # add some gaussian noise to tmp_embeds\n",
    "        tmp_embeds.data += th.randn_like(tmp_embeds.data)*0.005\n",
    "        tmp_embeds.requires_grad_(True)\n",
    "\n",
    "\n",
    "        # Then, calculate neuron_output\n",
    "        neuron_output = hook_model.embedded_forward(tmp_embeds)[0,:, neuron]\n",
    "        diversity_loss = cos(tmp_embeds[0], diverse_outputs[:d_ind]) #TODO, check if this is correct\n",
    "        loss = -neuron_output[-1] + diversity_loss.mean()\n",
    "\n",
    "        iterations_since_last_improvement += 1\n",
    "        # Save the highest activation\n",
    "        if neuron_output[-1] > largest_activation:\n",
    "            iterations_since_last_improvement = 0\n",
    "            largest_activation = neuron_output[-1]\n",
    "            largest_prompts[d_ind] = projected_index\n",
    "            print(f\"New largest activation: {largest_activation} | {model.to_string(projected_index)}\")\n",
    "\n",
    "        # Transfer the gradient to the continuous embedding space\n",
    "        prompt_embeds.grad, = th.autograd.grad(loss, [tmp_embeds])\n",
    "        \n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "    diverse_outputs[d_ind] = tmp_embeds.data[0,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50304, 768])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.W_E.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' approach elic mistakes Canadians'],\n",
       " [' categories restrict strategies Canadians'],\n",
       " [' culturally].)things bankers'],\n",
       " [' assessments**). mistakes consumers'],\n",
       " [' What rhet myths Davis'],\n",
       " [' concrete (“ skills Robertson'],\n",
       " [' comprom une actors shoppers'],\n",
       " [' Target impractical mistakes Canadians'],\n",
       " [' detailing mistakes Walmart citizens'],\n",
       " [' What limitations practices consumers']]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "largest_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0000, -1.0000, -1.0000, -1.0000],\n",
       "        [-1.0000, -1.0000, -1.0000, -1.0000]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos = th.nn.CosineSimilarity(dim=2)\n",
    "t = tmp_embeds[0]\n",
    "cos(t, th.stack((-t, -3*t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 768])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.stack((tmp_embeds[0], tmp_embeds[0])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\logan\\AppData\\Local\\Temp\\ipykernel_9372\\580220075.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\logan\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_9372\\\\580220075.py'</span>                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">cat</span><span style=\"font-weight: bold\">()</span>: argument <span style=\"color: #008000; text-decoration-color: #008000\">'tensors'</span> <span style=\"font-weight: bold\">(</span>position <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span> must be tuple of Tensors, not int\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\logan\\AppData\\Local\\Temp\\ipykernel_9372\\580220075.py\u001b[0m:\u001b[94m2\u001b[0m in \u001b[92m<module>\u001b[0m                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m'C:\\\\Users\\\\logan\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_9372\\\\580220075.py'\u001b[0m                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mTypeError: \u001b[0m\u001b[1;35mcat\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m: argument \u001b[32m'tensors'\u001b[0m \u001b[1m(\u001b[0mposition \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m must be tuple of Tensors, not int\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Combine the empty original, with the new prompt a\n",
    "original = th.cat(0)\n",
    "a = prompt_embeds[0,0,:]\n",
    "th.stack((a, original)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0001)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "ce = CrossEntropyLoss()\n",
    "ce(th.tensor([[10,0,0,99]]).float(), th.tensor([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\logan\\AppData\\Local\\Temp\\ipykernel_9372\\856322345.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\logan\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_9372\\\\856322345.py'</span>                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span>selected index k out of range\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\logan\\AppData\\Local\\Temp\\ipykernel_9372\\856322345.py\u001b[0m:\u001b[94m1\u001b[0m in \u001b[92m<module>\u001b[0m                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m'C:\\\\Users\\\\logan\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_9372\\\\856322345.py'\u001b[0m                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mRuntimeError: \u001b[0mselected index k out of range\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "th.tensor([[10,0,0,99]]).topk(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron 1: Embed: 0.47031161189079285 Discrete 0.47031161189079285 Distance N/A| Tokens: [' Pavel Rovinski\\n   \\nPavel Apolonovič Rovinski (1831']\n",
      "Neuron 1: Embed: 0.5555412769317627 Discrete 0.5666844248771667 Distance N/A| Tokens: [' eruption Pale Cris L reproductiveगariantgeryanch rot retain troublesome Ninaek Rovinski (1831']\n",
      "Neuron 1: Embed: 0.5436252355575562 Discrete 0.5354958176612854 Distance N/A| Tokens: [' frightened Pale Growth L accidentalumina scler recurrentarkpit diligence troublesome trivум Pegovinski [\\\\1831']\n",
      "Neuron 1: Embed: 0.9181833863258362 Discrete 0.9452750086784363 Distance N/A| Tokens: [' frightened candle catastrophe-------------------------------------- accidentalumin sclergeryCLpit 700 pad trivум ApplicationC Riv residency1831']\n",
      "Neuron 1: Embed: 0.5941134691238403 Discrete 0.51107257604599 Distance N/A| Tokens: [' happiness ashes disturbances lacterialорmonary pkgCLcycl 700 pad necklaceş ApplicationC Riv residency1831']\n",
      "Neuron 1: Embed: 0.6064326167106628 Discrete 0.6191575527191162 Distance N/A| Tokens: [' treaty healing GenAtopeniaорmonaryperatureCL ADC 608 pad recallsş DISTRICTC Riv residency1831']\n",
      "Neuron 1: Embed: 0.5291111469268799 Discrete 0.5832246541976929 Distance N/A| Tokens: [' prevents healing GenAt EngineорphenperatureAPH ADC 608 Byte recallsş DISTRICTC Riv residency1831']\n",
      "Neuron 1: Embed: 0.5613997578620911 Discrete 0.6677514910697937 Distance N/A| Tokens: [' abolition cure GenSpanish Enginestudphen ChildAPHLP flaps Byte recallsş Val*>( stri residency1831']\n",
      "Neuron 1: Embed: 0.6380972266197205 Discrete 0.9355320930480957 Distance N/A| Tokens: [' abolition cure GenSpanish Enginestudphen factorizationarkLP flaps Byte snippetek Eight=` entriesestead1831']\n",
      "Neuron 1: Embed: 0.9731943011283875 Discrete 0.9762405753135681 Distance N/A| Tokens: [' abolition cure EvolutionSpanishopeniaStructurephen factorization FarmLP Memoryennis snippetek Linked=` Or Kaz1831']\n",
      "Neuron 1: Embed: 0.7246442437171936 Discrete 0.7400828003883362 Distance N/A| Tokens: [' prevents cureOOKSpanishplasiastudphen psychologist FarmLP Truck Pad recalls consisting Arc receiver Or Kaz1831']\n",
      "Neuron 1: Embed: 0.8180707097053528 Discrete 0.8386719822883606 Distance N/A| Tokens: [' polynomial filos HansSpanishaucomaIsraelzheimer surfactantK Tam Truck Pad recalls consisting Arcrr Or Kaz1831']\n",
      "Neuron 1: Embed: 0.7536217570304871 Discrete 0.7726160883903503 Distance N/A| Tokens: ['operations solids HansSpanishplasiaIsrael Disorders surfactantK Tam Truck Pad Directors consistingpatch?? entries Kaz1831']\n",
      "Neuron 1: Embed: 0.7163833379745483 Discrete 0.72981196641922 Distance N/A| Tokens: ['operations solids Hans anatomicaldegenerateIsrael Disorders surfactantK Bath Memory Jeff Directors consistingpatch från entries Kaz1831']\n",
      "Neuron 1: Embed: 0.8466748595237732 Discrete 0.7059684991836548 Distance N/A| Tokens: [' borough solids Hans anatomicalylaseIsraelzheimer additive ALT Bath shortcomings Jed Yearerv prison från Shel Kaz1831']\n",
      "Neuron 1: Embed: 0.6897984147071838 Discrete 0.6886132955551147 Distance N/A| Tokens: [' borough solids Hans anatomicaldegenerateIsrael Haus additive ALT Bath shortcomings Pad Yearerv prison från Shel Kaz1831']\n",
      "Neuron 1: Embed: 0.8151138424873352 Discrete 0.8363693356513977 Distance N/A| Tokens: ['operations Funasa charsroscopic解 Hausementia ALT Б Border Pad birthdayerv prison 328 Shel Kaz1831']\n",
      "Neuron 1: Embed: 1.022383213043213 Discrete 0.8512213826179504 Distance N/A| Tokens: [' removes FunῖSpanishroscopic解 Haus。Sports Kick shortcomings Conn Caseserv prison 328 Shel Kaz1831']\n",
      "Neuron 1: Embed: 1.0567240715026855 Discrete 1.1027930974960327 Distance N/A| Tokens: [' removes SculStructureSpanishroscopic解 Hausementia Farm Kick Pad Conn Caseserv prison 328 Shel Kaz1831']\n",
      "Neuron 1: Embed: 0.7629280090332031 Discrete 0.6402121782302856 Distance N/A| Tokens: [' removes HullStructure charsroscopic解 Haus。 VB polyethylene shortcomings J ¶¶erv prison 328 Shel Kaz1831']\n",
      "Neuron 1: Embed: 0.6571115851402283 Discrete 0.7309775948524475 Distance N/A| Tokens: [' removes Hullmology charsrospective解 Haus。 Boost polyethylene 768 J ¶¶erv prison 328 Shel Kaz1831']\n",
      "Neuron 1: Embed: 0.6839442849159241 Discrete 0.6839442849159241 Distance N/A| Tokens: [' removes HullmologyHungrospective解 Haus。 Boost Kentucky 768 J chairserv prison 328 Shel Kaz1831']\n",
      "Neuron 1: Embed: 0.9079313278198242 Discrete 0.9079313278198242 Distance N/A| Tokens: [' removes HullmologyHungaucomaosarcoma Hausementia Boost Kentuckypixel J chairserv jail 328 Shel Kaz1831']\n",
      "Neuron 1: Embed: 0.8720077872276306 Discrete 0.9611375331878662 Distance N/A| Tokens: ['trees cureême charsaucomaFail Haus dispense VB Kentucky Window J chairserv jail 328 Shel Kaz1831']\n",
      "Neuron 1: Embed: 0.9244800209999084 Discrete 0.8449479937553406 Distance N/A| Tokens: ['Size cureulas chars congenitalSpect Esc dispense Bath Metal Window Pad chairserv jail 328 Shel Kaz1831']\n",
      "Neuron 1: Embed: 0.8205773234367371 Discrete 0.7698244452476501 Distance N/A| Tokens: ['Size Hullologie chars congenitalSpect Esc dispense FREE Metal Window Conn thruerv jail 328 Shel Kaz1831']\n",
      "Neuron 1: Embed: 0.8583725094795227 Discrete 0.8357240557670593 Distance N/A| Tokens: [' spoiled HullologieSpanish congenitalSpect glo dispense FREE Kentucky Truck J thruerv jail 328 Shel Kaz1831']\n",
      "Neuron 1: Embed: 0.6457663774490356 Discrete 0.6222229599952698 Distance N/A| Tokens: [' spoiled Hullciasívotypicosarcoma glointeractingAsp Custom Window J 457erv jail 328 Shel Kaz1831']\n",
      "Neuron 1: Embed: 1.0757744312286377 Discrete 1.0781567096710205 Distance N/A| Tokens: [' spoiled Elsaciasacterialotypicosarcoma atmosp dispense Bath Kentucky Truck J Prizeerv jail 328 Shel Kaz1831']\n",
      "Neuron 1: Embed: 0.9252602458000183 Discrete 0.9252602458000183 Distance N/A| Tokens: [' spoiled beesorganismsacterialotypic Pure atmospinteracting Bath Kentucky Patch PadViewserv jail 328 Shel Kaz1831']\n",
      "Neuron 1: Embed: 0.6953060030937195 Discrete 0.7190089225769043 Distance N/A| Tokens: [' spoiled beesomenacterialotypic Pureosaurinteracting Bath Kentucky Pad J™erv jail 328 Shel Kaz1831']\n",
      "Neuron 1: Embed: 0.7700236439704895 Discrete 0.8129320740699768 Distance N/A| Tokens: ['Size ElsaorganismsHungotypicosarcoma haunted dispense Bath plated Pad rid™erv jail 328 Shel Kaz1831']\n",
      "Neuron 1: Embed: 0.7205938100814819 Discrete 0.6684034466743469 Distance N/A| Tokens: [' spoiled beesomenHungotypic idiopathicosaur dispense FREE plated mini Conn ¡erv jail 328 Shel Kaz1831']\n",
      "Neuron 1: Embed: 0.9229335188865662 Discrete 0.9399654865264893 Distance N/A| Tokens: ['operations beesomenSpanishotypic idiopathic haunted dispenseocolate Б Truck Nar thruerv jail 328 Shel Kaz1831']\n",
      "Neuron 1: Embed: 0.9546270966529846 Discrete 0.9321777820587158 Distance N/A| Tokens: ['operations aimsयacterialientos Pure hauntedinteracting FREE Б Dennis Ern thruerv jail 328 Shel Kaz1831']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\logan\\AppData\\Local\\Temp\\ipykernel_16424\\2169432603.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">23</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\logan\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_16424\\\\2169432603.py'</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\logan\\AppData\\Local\\Temp\\ipykernel_16424\\2169432603.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">23</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;listcomp&gt;</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\logan\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_16424\\\\2169432603.py'</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\logan\\AppData\\Local\\Temp\\ipykernel_16424\\2169432603.py\u001b[0m:\u001b[94m23\u001b[0m in \u001b[92m<module>\u001b[0m                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m'C:\\\\Users\\\\logan\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_16424\\\\2169432603.py'\u001b[0m                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\logan\\AppData\\Local\\Temp\\ipykernel_16424\\2169432603.py\u001b[0m:\u001b[94m23\u001b[0m in \u001b[92m<listcomp>\u001b[0m                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m'C:\\\\Users\\\\logan\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_16424\\\\2169432603.py'\u001b[0m                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for neuron in range(100):\n",
    "\n",
    "# init_text = \"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\"\n",
    "init_text = \" the injuries England have\"\n",
    "# init_text = \" 1 a ;\"\n",
    "# init_text = ''' Pavel Rovinski\n",
    "layer=6\n",
    "neuron = 493\n",
    "\n",
    "# Pavel Apolonovič Rovinski (1831'''\n",
    "#TODO make work w/ 1 token\n",
    "init_tokens = model.to_tokens(init_text, prepend_bos=False)\n",
    "prompt_embeds = th.nn.Parameter(model.embed(init_tokens)).detach()\n",
    "prompt_embeds.requires_grad_(True)\n",
    "\n",
    "_, seq, _ = prompt_embeds.shape\n",
    "neuron = 1\n",
    "\n",
    "# input_optimizer = torch.optim.AdamW([prompt_embeds], lr=lr, weight_decay=weight_decay)\n",
    "# optim = th.optim.SGD([prompt_embeds], lr=0.2)\n",
    "optim = th.optim.AdamW([prompt_embeds], lr=.1, weight_decay=0.01)\n",
    "epochs = 201\n",
    "for i in range(epochs):\n",
    "    # First, project into the embedding matrix\n",
    "    with th.no_grad():\n",
    "        projected_index = th.stack([(hook_model.embed_weights@prompt_embeds[0,i,:]).argmax() for i in range(seq)]).unsqueeze(0)\n",
    "        projected_embeds = model.embed(projected_index)\n",
    "\n",
    "    # Create a temp embedding that is detached from the graph, but has the same data as the projected embedding\n",
    "    tmp_embeds = prompt_embeds.detach().clone()\n",
    "    tmp_embeds.data = projected_embeds.data\n",
    "    # add some gaussian noise to tmp_embeds\n",
    "    # tmp_embeds.data += th.randn_like(tmp_embeds.data)*0.2\n",
    "    tmp_embeds.requires_grad_(True)\n",
    "\n",
    "\n",
    "    # Then, calculate neuron_output\n",
    "    neuron_output = hook_model.embedded_forward(tmp_embeds)[0,:, neuron]\n",
    "    loss = -neuron_output.max()\n",
    "\n",
    "    # Transfer the gradient to the continuous embedding space\n",
    "    prompt_embeds.grad, = th.autograd.grad(loss, [tmp_embeds])\n",
    "    \n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "\n",
    "\n",
    "    # neuron_output = hook_model.embedded_forward(embedded_tokens)[0,:, neuron].mean()\n",
    "\n",
    "    # embed_weights_norm = hook_model.embed_weights / hook_model.embed_weights.norm(dim=1).unsqueeze(1)\n",
    "    # token_embed_sizes = embedded_tokens[0,:,:].norm(dim=1)\n",
    "    # distance = th.stack([(1-(embed_weights_norm@(embedded_tokens[0,i,:] / token_embed_sizes[i]))).min() for i in range(seq)])\n",
    "    # # distance = th.stack([(1-hook_model.embed_weights@embedded_tokens[0,i,:]).min() for i in range(seq)])\n",
    "    # # Distance from the embedding matrix\n",
    "    # # dist = th.norm(hook_model.embed_weights@(embedded_tokens[0,:,:]).T - embedded_tokens[0,:,:].T, dim=1).mean()\n",
    "    \n",
    "    # loss = -neuron_output \n",
    "    # # loss = -neuron_output + distance.mean()*10\n",
    "    # # loss = distance.mean()*10\n",
    "    # loss.backward()\n",
    "    # optim.step()\n",
    "    # optim.zero_grad()\n",
    "    if i % 5 == 0:\n",
    "        with th.no_grad():\n",
    "            # Find the maximum similarity between each embedded_token, and the embedding matrix\n",
    "            # Picking that token's embedding is equivalent to projecting onto the closest vector in the embedding matrix\n",
    "            new_tokens = th.stack([(hook_model.embed_weights@prompt_embeds[0,i,:]).argmax() for i in range(seq)]).unsqueeze(0)\n",
    "            discrete_neuron_output = hook_model.embedded_forward(model.embed(new_tokens))[0,:, neuron]\n",
    "            # print(f\"Neuron {neuron}: Embed: {neuron_output.mean()} Discrete {discrete_neuron_output.mean()} Distance {distance.mean().item()}| Tokens: {model.to_string(new_tokens)}\")\n",
    "            print(f\"Neuron {neuron}: Embed: {neuron_output.max()} Discrete {discrete_neuron_output.max()} Distance N/A| Tokens: {model.to_string(new_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 253, 9478, 5854,  452]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 768])\n",
      "torch.Size([1, 6, 768])\n",
      "tensor(0.0019, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(tmp_embeds.data.shape)\n",
    "print(prompt_embeds.data.shape)\n",
    "print(loss)\n",
    "embedded_tokens.grad, = th.autograd.grad(loss, [tmp_embeds])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50304])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(hook_model.embed_weights@prompt_embeds[0,i,:]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\logan\\AppData\\Local\\Temp\\ipykernel_16424\\913517730.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\logan\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_16424\\\\913517730.py'</span>                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\logan\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1269</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__getattr__</span>       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1266 │   │   │   </span>modules = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__dict__</span>[<span style=\"color: #808000; text-decoration-color: #808000\">'_modules'</span>]                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1267 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> name <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> modules:                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1268 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> modules[name]                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1269 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">AttributeError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"'{}' object has no attribute '{}'\"</span>.format(                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1270 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>).<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__name__</span>, name))                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1271 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1272 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__setattr__</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, name: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>, value: Union[Tensor, <span style=\"color: #808000; text-decoration-color: #808000\">'Module'</span>]) -&gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AttributeError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Embed'</span> object has no attribute <span style=\"color: #008000; text-decoration-color: #008000\">'requires_grad'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\logan\\AppData\\Local\\Temp\\ipykernel_16424\\913517730.py\u001b[0m:\u001b[94m1\u001b[0m in \u001b[92m<module>\u001b[0m                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m'C:\\\\Users\\\\logan\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_16424\\\\913517730.py'\u001b[0m                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\logan\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m:\u001b[94m1269\u001b[0m in \u001b[92m__getattr__\u001b[0m       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1266 \u001b[0m\u001b[2m│   │   │   \u001b[0mmodules = \u001b[96mself\u001b[0m.\u001b[91m__dict__\u001b[0m[\u001b[33m'\u001b[0m\u001b[33m_modules\u001b[0m\u001b[33m'\u001b[0m]                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1267 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m name \u001b[95min\u001b[0m modules:                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1268 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m modules[name]                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1269 \u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mAttributeError\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33m'\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m'\u001b[0m\u001b[33m object has no attribute \u001b[0m\u001b[33m'\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m'\u001b[0m\u001b[33m\"\u001b[0m.format(                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1270 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mtype\u001b[0m(\u001b[96mself\u001b[0m).\u001b[91m__name__\u001b[0m, name))                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1271 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1272 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m__setattr__\u001b[0m(\u001b[96mself\u001b[0m, name: \u001b[96mstr\u001b[0m, value: Union[Tensor, \u001b[33m'\u001b[0m\u001b[33mModule\u001b[0m\u001b[33m'\u001b[0m]) -> \u001b[94mNone\u001b[0m:             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mAttributeError: \u001b[0m\u001b[32m'Embed'\u001b[0m object has no attribute \u001b[32m'requires_grad'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron 20: Embed: -0.21370284259319305 Discrete -0.21370284259319305| Tokens: ['the quick brown 1 2 3 4 5 7 8 9 0 - - -']\n",
      "Neuron 20: Embed: -0.21370284259319305 Discrete -0.21370284259319305| Tokens: ['the quick brown 1 2 3 4 5 7 8 9 0 - - -']\n",
      "Neuron 20: Embed: -0.21370284259319305 Discrete -0.21370284259319305| Tokens: ['the quick brown 1 2 3 4 5 7 8 9 0 - - -']\n",
      "Neuron 20: Embed: -0.21370284259319305 Discrete -0.21370284259319305| Tokens: ['the quick brown 1 2 3 4 5 7 8 9 0 - - -']\n",
      "Neuron 20: Embed: -0.21370284259319305 Discrete -0.21370284259319305| Tokens: ['the quick brown 1 2 3 4 5 7 8 9 0 - - -']\n",
      "Neuron 20: Embed: -0.21370284259319305 Discrete -0.21370284259319305| Tokens: ['the quick brown 1 2 3 4 5 7 8 9 0 - - -']\n",
      "Neuron 20: Embed: -0.21370284259319305 Discrete -0.21370284259319305| Tokens: ['the quick brown 1 2 3 4 5 7 8 9 0 - - -']\n",
      "Neuron 20: Embed: -0.21370284259319305 Discrete -0.21370284259319305| Tokens: ['the quick brown 1 2 3 4 5 7 8 9 0 - - -']\n",
      "Neuron 20: Embed: -0.21370284259319305 Discrete -0.21370284259319305| Tokens: ['the quick brown 1 2 3 4 5 7 8 9 0 - - -']\n",
      "Neuron 20: Embed: -0.21370284259319305 Discrete -0.21370284259319305| Tokens: ['the quick brown 1 2 3 4 5 7 8 9 0 - - -']\n",
      "Neuron 20: Embed: -0.21370284259319305 Discrete -0.21370284259319305| Tokens: ['the quick brown 1 2 3 4 5 7 8 9 0 - - -']\n",
      "Neuron 20: Embed: -0.21370284259319305 Discrete -0.21370284259319305| Tokens: ['the quick brown 1 2 3 4 5 7 8 9 0 - - -']\n",
      "Neuron 20: Embed: -0.21370284259319305 Discrete -0.21370284259319305| Tokens: ['the quick brown 1 2 3 4 5 7 8 9 0 - - -']\n",
      "Neuron 20: Embed: -0.21370284259319305 Discrete -0.21370284259319305| Tokens: ['the quick brown 1 2 3 4 5 7 8 9 0 - - -']\n",
      "Neuron 20: Embed: -0.21370284259319305 Discrete -0.21370284259319305| Tokens: ['the quick brown 1 2 3 4 5 7 8 9 0 - - -']\n",
      "Neuron 20: Embed: -0.21370284259319305 Discrete -0.21370284259319305| Tokens: ['the quick brown 1 2 3 4 5 7 8 9 0 - - -']\n",
      "Neuron 20: Embed: -0.21370284259319305 Discrete -0.21370284259319305| Tokens: ['the quick brown 1 2 3 4 5 7 8 9 0 - - -']\n",
      "Neuron 20: Embed: -0.21370284259319305 Discrete -0.21370284259319305| Tokens: ['the quick brown 1 2 3 4 5 7 8 9 0 - - -']\n",
      "Neuron 20: Embed: -0.21370284259319305 Discrete -0.21370284259319305| Tokens: ['the quick brown 1 2 3 4 5 7 8 9 0 - - -']\n",
      "Neuron 20: Embed: -0.21370284259319305 Discrete -0.21370284259319305| Tokens: ['the quick brown 1 2 3 4 5 7 8 9 0 - - -']\n",
      "Neuron 20: Embed: -0.21370284259319305 Discrete -0.21370284259319305| Tokens: ['the quick brown 1 2 3 4 5 7 8 9 0 - - -']\n",
      "Neuron 20: Embed: -0.21370284259319305 Discrete -0.21370284259319305| Tokens: ['the quick brown 1 2 3 4 5 7 8 9 0 - - -']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\logan\\AppData\\Local\\Temp\\ipykernel_27320\\3650086024.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">8</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\logan\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_27320\\\\3650086024.py'</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\logan\\AppData\\Local\\Temp\\ipykernel_27320\\2695780327.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">17</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">embedded_forward</span>           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\logan\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_27320\\\\2695780327.py'</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\logan\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1194</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1191 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1192 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1193 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1194 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1195 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1196 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1197 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\logan\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">204</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">201 │   # with Any as TorchScript expects a more precise type</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">202 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>):                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">203 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> module <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>:                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>204 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span> = module(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>)                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">205 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">206 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">207 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">append</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, module: Module) -&gt; <span style=\"color: #808000; text-decoration-color: #808000\">'Sequential'</span>:                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\logan\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1194</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1191 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1192 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1193 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1194 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1195 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1196 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1197 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\logan\\miniconda3\\lib\\site-packages\\transformer_lens\\components.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">705</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">702 │   │   </span>resid_pre = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.hook_resid_pre(resid_pre)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># [batch, pos, d_model]</span>                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">703 │   │   </span>normalized_resid_pre = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.ln1(resid_pre)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">704 │   │   </span>attn_out = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.hook_attn_out(                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>705 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.attn(                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">706 │   │   │   │   </span>normalized_resid_pre,                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">707 │   │   │   │   </span>shortformer_pos_embed=shortformer_pos_embed,                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">708 │   │   │   │   </span>past_kv_cache_entry=past_kv_cache_entry,                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\logan\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1194</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1191 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1192 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1193 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1194 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1195 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1196 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1197 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\logan\\miniconda3\\lib\\site-packages\\transformer_lens\\components.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">396</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">393 │   │   │   </span>kv_cache_pos_offset = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">394 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">395 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.cfg.positional_embedding_type == <span style=\"color: #808000; text-decoration-color: #808000\">\"rotary\"</span>:                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>396 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>q, k = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.rotary_rotate_qk(q, k, kv_cache_pos_offset)                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">397 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">398 │   │   </span>attn_scores = (                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">399 │   │   │   </span>einsum(                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\logan\\miniconda3\\lib\\site-packages\\transformer_lens\\components.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">524</span> in                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">rotary_rotate_qk</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">521 │   │   </span>TT[T.batch, T.pos, T.head_index, T.d_head],                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">522 │   </span>]:                                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">523 │   │   # We first apply standard q and k calculation</span>                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>524 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>q = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.hook_rot_q(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.apply_rotary(q, past_kv_pos_offset))                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">525 │   │   </span>k = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.hook_rot_k(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.apply_rotary(k))                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">526 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> q, k                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">527 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\logan\\miniconda3\\lib\\site-packages\\transformer_lens\\components.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">575</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">apply_rotary</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">572 │   │   </span>x_pos = x.size(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>)                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">573 │   │   </span>x_rot = x[..., : <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.cfg.rotary_dim]                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">574 │   │   </span>x_pass = x[..., <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.cfg.rotary_dim :]                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>575 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>x_flip = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.rotate_every_two(x_rot)                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">576 │   │   </span>x_rotated = (                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">577 │   │   │   </span>x_rot                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">578 │   │   │   </span>* <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.rotary_cos[past_kv_pos_offset : past_kv_pos_offset + x_pos, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>, :]    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\logan\\miniconda3\\lib\\site-packages\\transformer_lens\\components.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">560</span> in                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">rotate_every_two</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">557 │   │   </span>rot_x = x.clone()                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">558 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.cfg.original_architecture == <span style=\"color: #808000; text-decoration-color: #808000\">\"GPTNeoXForCausalLM\"</span>:                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">559 │   │   │   </span>n = x.size(-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>) // <span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>560 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>rot_x[..., :n] = -x[..., n:]                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">561 │   │   │   </span>rot_x[..., n:] = x[..., :n]                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">562 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">563 │   │   │   </span>rot_x[..., ::<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>] = -x[..., <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>::<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>]                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\logan\\AppData\\Local\\Temp\\ipykernel_27320\\3650086024.py\u001b[0m:\u001b[94m8\u001b[0m in \u001b[92m<module>\u001b[0m                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m'C:\\\\Users\\\\logan\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_27320\\\\3650086024.py'\u001b[0m                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\logan\\AppData\\Local\\Temp\\ipykernel_27320\\2695780327.py\u001b[0m:\u001b[94m17\u001b[0m in \u001b[92membedded_forward\u001b[0m           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m'C:\\\\Users\\\\logan\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_27320\\\\2695780327.py'\u001b[0m                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\logan\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m:\u001b[94m1194\u001b[0m in \u001b[92m_call_impl\u001b[0m        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1191 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1192 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1193 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1194 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1195 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1196 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1197 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\logan\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m:\u001b[94m204\u001b[0m in \u001b[92mforward\u001b[0m         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m201 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# with Any as TorchScript expects a more precise type\u001b[0m                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m202 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mforward\u001b[0m(\u001b[96mself\u001b[0m, \u001b[96minput\u001b[0m):                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m203 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m module \u001b[95min\u001b[0m \u001b[96mself\u001b[0m:                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m204 \u001b[2m│   │   │   \u001b[0m\u001b[96minput\u001b[0m = module(\u001b[96minput\u001b[0m)                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m205 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96minput\u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m206 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m207 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mappend\u001b[0m(\u001b[96mself\u001b[0m, module: Module) -> \u001b[33m'\u001b[0m\u001b[33mSequential\u001b[0m\u001b[33m'\u001b[0m:                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\logan\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m:\u001b[94m1194\u001b[0m in \u001b[92m_call_impl\u001b[0m        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1191 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1192 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1193 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1194 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1195 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1196 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1197 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\logan\\miniconda3\\lib\\site-packages\\transformer_lens\\components.py\u001b[0m:\u001b[94m705\u001b[0m in \u001b[92mforward\u001b[0m        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m702 \u001b[0m\u001b[2m│   │   \u001b[0mresid_pre = \u001b[96mself\u001b[0m.hook_resid_pre(resid_pre)  \u001b[2m# [batch, pos, d_model]\u001b[0m                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m703 \u001b[0m\u001b[2m│   │   \u001b[0mnormalized_resid_pre = \u001b[96mself\u001b[0m.ln1(resid_pre)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m704 \u001b[0m\u001b[2m│   │   \u001b[0mattn_out = \u001b[96mself\u001b[0m.hook_attn_out(                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m705 \u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.attn(                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m706 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mnormalized_resid_pre,                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m707 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mshortformer_pos_embed=shortformer_pos_embed,                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m708 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mpast_kv_cache_entry=past_kv_cache_entry,                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\logan\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m:\u001b[94m1194\u001b[0m in \u001b[92m_call_impl\u001b[0m        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1191 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1192 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1193 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1194 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1195 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1196 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1197 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\logan\\miniconda3\\lib\\site-packages\\transformer_lens\\components.py\u001b[0m:\u001b[94m396\u001b[0m in \u001b[92mforward\u001b[0m        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m393 \u001b[0m\u001b[2m│   │   │   \u001b[0mkv_cache_pos_offset = \u001b[94m0\u001b[0m                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m394 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m395 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.cfg.positional_embedding_type == \u001b[33m\"\u001b[0m\u001b[33mrotary\u001b[0m\u001b[33m\"\u001b[0m:                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m396 \u001b[2m│   │   │   \u001b[0mq, k = \u001b[96mself\u001b[0m.rotary_rotate_qk(q, k, kv_cache_pos_offset)                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m397 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m398 \u001b[0m\u001b[2m│   │   \u001b[0mattn_scores = (                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m399 \u001b[0m\u001b[2m│   │   │   \u001b[0meinsum(                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\logan\\miniconda3\\lib\\site-packages\\transformer_lens\\components.py\u001b[0m:\u001b[94m524\u001b[0m in                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mrotary_rotate_qk\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m521 \u001b[0m\u001b[2m│   │   \u001b[0mTT[T.batch, T.pos, T.head_index, T.d_head],                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m522 \u001b[0m\u001b[2m│   \u001b[0m]:                                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m523 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# We first apply standard q and k calculation\u001b[0m                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m524 \u001b[2m│   │   \u001b[0mq = \u001b[96mself\u001b[0m.hook_rot_q(\u001b[96mself\u001b[0m.apply_rotary(q, past_kv_pos_offset))                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m525 \u001b[0m\u001b[2m│   │   \u001b[0mk = \u001b[96mself\u001b[0m.hook_rot_k(\u001b[96mself\u001b[0m.apply_rotary(k))                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m526 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m q, k                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m527 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\logan\\miniconda3\\lib\\site-packages\\transformer_lens\\components.py\u001b[0m:\u001b[94m575\u001b[0m in \u001b[92mapply_rotary\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m572 \u001b[0m\u001b[2m│   │   \u001b[0mx_pos = x.size(\u001b[94m1\u001b[0m)                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m573 \u001b[0m\u001b[2m│   │   \u001b[0mx_rot = x[..., : \u001b[96mself\u001b[0m.cfg.rotary_dim]                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m574 \u001b[0m\u001b[2m│   │   \u001b[0mx_pass = x[..., \u001b[96mself\u001b[0m.cfg.rotary_dim :]                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m575 \u001b[2m│   │   \u001b[0mx_flip = \u001b[96mself\u001b[0m.rotate_every_two(x_rot)                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m576 \u001b[0m\u001b[2m│   │   \u001b[0mx_rotated = (                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m577 \u001b[0m\u001b[2m│   │   │   \u001b[0mx_rot                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m578 \u001b[0m\u001b[2m│   │   │   \u001b[0m* \u001b[96mself\u001b[0m.rotary_cos[past_kv_pos_offset : past_kv_pos_offset + x_pos, \u001b[94mNone\u001b[0m, :]    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\logan\\miniconda3\\lib\\site-packages\\transformer_lens\\components.py\u001b[0m:\u001b[94m560\u001b[0m in                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mrotate_every_two\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m557 \u001b[0m\u001b[2m│   │   \u001b[0mrot_x = x.clone()                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m558 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.cfg.original_architecture == \u001b[33m\"\u001b[0m\u001b[33mGPTNeoXForCausalLM\u001b[0m\u001b[33m\"\u001b[0m:                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m559 \u001b[0m\u001b[2m│   │   │   \u001b[0mn = x.size(-\u001b[94m1\u001b[0m) // \u001b[94m2\u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m560 \u001b[2m│   │   │   \u001b[0mrot_x[..., :n] = -x[..., n:]                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m561 \u001b[0m\u001b[2m│   │   │   \u001b[0mrot_x[..., n:] = x[..., :n]                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m562 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m563 \u001b[0m\u001b[2m│   │   │   \u001b[0mrot_x[..., ::\u001b[94m2\u001b[0m] = -x[..., \u001b[94m1\u001b[0m::\u001b[94m2\u001b[0m]                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for neuron in range(100):\n",
    "neuron=20\n",
    "init_text = \"the quick brown 1 2 3 4 5 7 8 9 0 - - -\"\n",
    "init_tokens = model.to_tokens(init_text, prepend_bos=False)\n",
    "embedded_tokens = th.nn.Parameter(model.embed(init_tokens))\n",
    "iters = 200\n",
    "for i in range(iters+1):\n",
    "    neuron_output = hook_model.embedded_forward(embedded_tokens)[0,:, neuron].mean()\n",
    "    optim = th.optim.Adam([embedded_tokens], lr=0.1)\n",
    "    loss = -neuron_output\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    if i % 1 == 0:\n",
    "        _, seq, _ = embedded_tokens.shape\n",
    "        # Find the maximum similarity between each embedded_token, and the embedding matrix\n",
    "        # Picking that token's embedding is equivalent to projecting onto the closest vector in the embedding matrix\n",
    "        new_tokens = th.stack([(hook_model.embed_weights@embedded_tokens[0,i,:]).argmax() for i in range(seq)]).unsqueeze(0)\n",
    "        with th.no_grad():\n",
    "            embedded_tokens.copy_(model.embed(new_tokens))\n",
    "        discrete_neuron_output = hook_model.embedded_forward(embedded_tokens)[0,:, neuron].mean()\n",
    "        print(f\"Neuron {neuron}: Embed: {neuron_output} Discrete {discrete_neuron_output}| Tokens: {model.to_string(new_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# for neuron in range(100):\u001b[39;00m\n\u001b[0;32m      3\u001b[0m init_text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m init_tokens \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto_tokens(init_text, prepend_bos\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m embedded_tokens \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mParameter(model\u001b[39m.\u001b[39membed(init_tokens))\n\u001b[0;32m      6\u001b[0m _, seq, _ \u001b[39m=\u001b[39m embedded_tokens\u001b[39m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# for neuron in range(100):\n",
    "\n",
    "init_text = \"1\"\n",
    "init_tokens = model.to_tokens(init_text, prepend_bos=False)\n",
    "embedded_tokens = th.nn.Parameter(model.embed(init_tokens))\n",
    "_, seq, _ = embedded_tokens.shape\n",
    "\n",
    "\n",
    "optim = th.optim.SGD([embedded_tokens], lr=1.2)\n",
    "epochs = 201\n",
    "for i in range(epochs):\n",
    "    neuron_output = hook_model.embedded_forward(embedded_tokens)[0,:, neuron].mean()\n",
    "\n",
    "    embed_weights_norm = hook_model.embed_weights / hook_model.embed_weights.norm(dim=1).unsqueeze(1)\n",
    "    token_embed_sizes = embedded_tokens[0,:,:].norm(dim=1)\n",
    "    distance = th.stack([(1-(embed_weights_norm@(embedded_tokens[0,i,:] / token_embed_sizes[i]))).min() for i in range(seq)])\n",
    "    # distance = th.stack([(1-hook_model.embed_weights@embedded_tokens[0,i,:]).min() for i in range(seq)])\n",
    "    # Distance from the embedding matrix\n",
    "    # dist = th.norm(hook_model.embed_weights@(embedded_tokens[0,:,:]).T - embedded_tokens[0,:,:].T, dim=1).mean()\n",
    "    \n",
    "    loss = -neuron_output \n",
    "    # loss = -neuron_output + distance.mean()*10\n",
    "    # loss = distance.mean()*10\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    if i % 5 == 0:\n",
    "        # Find the maximum similarity between each embedded_token, and the embedding matrix\n",
    "        # Picking that token's embedding is equivalent to projecting onto the closest vector in the embedding matrix\n",
    "        new_tokens = th.stack([(hook_model.embed_weights@embedded_tokens[0,i,:]).argmax() for i in range(seq)]).unsqueeze(0)\n",
    "        discrete_neuron_output = hook_model.embedded_forward(model.embed(new_tokens))[0,:, neuron].mean()\n",
    "        print(f\"Neuron {neuron}: Embed: {neuron_output} Discrete {discrete_neuron_output} Distance {distance.mean().item()}| Tokens: {model.to_string(new_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1070, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron = hook_model(new_tokens)\n",
    "neuron[0,:, 0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0954e+01, -1.8866e+01,  2.2489e+00, -1.7445e+01,  3.9778e+01,\n",
       "         3.5294e-01, -2.3105e+01, -6.6568e+00,  1.4689e+01,  3.8227e+00,\n",
       "         1.0523e+02, -1.0434e+01, -2.2643e+01, -1.7965e+01,  1.3834e+01,\n",
       "         9.6289e-02,  1.2760e+01,  6.6267e+00,  5.0471e+00,  1.7947e+01,\n",
       "        -9.4464e+00, -1.5378e+01,  7.7645e+00, -1.9317e+01,  4.5725e-01,\n",
       "         3.5412e+01,  5.6189e+00, -2.9805e+01,  2.7511e+01,  3.2154e+01,\n",
       "        -8.2566e+00, -1.5262e+01,  2.4438e+01,  1.9494e+01,  9.8379e+00,\n",
       "         1.4377e+01, -1.8982e+01,  8.1687e+00, -3.9900e+01, -1.4327e+01,\n",
       "         2.0315e+01,  1.3450e+00,  2.6656e+01,  1.6861e+01,  7.9241e-01,\n",
       "        -5.9983e-01,  5.8152e+00,  4.4811e+00, -4.0319e+00, -2.1985e+01,\n",
       "         4.5013e+00, -1.5701e+01,  1.6422e+01,  2.5177e+01, -8.7880e+00,\n",
       "        -4.9935e+00, -2.0194e+01,  1.5267e+01, -2.3336e+01,  6.4996e+00,\n",
       "        -2.6224e+01, -1.3853e+01, -2.7448e+01,  8.6077e+00, -2.4899e+00,\n",
       "         3.0491e+01, -2.8046e+00, -1.2375e+01,  3.4557e+01, -1.4696e+01,\n",
       "         1.7291e+00, -1.2190e+01, -2.5208e+01,  3.2940e+01, -3.8009e+00,\n",
       "         4.7655e+01,  3.2972e+00,  3.2177e+01,  1.9009e+01, -5.9638e+00,\n",
       "        -1.5690e+01,  4.6775e+00, -8.7995e+00, -2.9574e+01,  3.8161e+01,\n",
       "        -5.9261e-01,  4.5360e+00, -6.1659e+00, -1.4719e+01, -7.0090e-01,\n",
       "        -1.6279e+01,  4.7381e+00, -4.2280e+01, -1.1751e+01,  4.9489e+00,\n",
       "         1.0271e+01, -8.1296e+00, -5.3198e+00, -1.1889e+01, -8.6032e+00,\n",
       "        -1.3830e+01, -9.2470e-01, -6.7377e+00,  3.1554e+01, -4.1933e+01,\n",
       "         1.7207e+01, -4.7278e+00, -2.7425e+00,  1.8836e+01, -7.1766e+00,\n",
       "         6.3503e+01,  5.7142e+00, -5.2329e+01,  4.6111e+00,  1.1680e+01,\n",
       "        -9.2442e+00,  6.0492e+00,  1.1391e+01, -5.9927e+00, -1.2063e+01,\n",
       "         4.2982e-01, -2.8595e+00, -1.6879e+01,  1.5844e+01, -2.8881e+01,\n",
       "        -1.0093e+01,  1.2448e+01, -6.4200e+00, -2.7102e+01, -5.4902e+00,\n",
       "         1.4250e+01,  5.8124e+00, -5.2909e+00,  1.4666e+01, -2.0749e+01,\n",
       "        -2.1823e+01,  6.8750e+00, -9.5850e+00, -3.1306e+01, -8.2913e+00,\n",
       "        -2.9458e+01,  1.9483e+01, -1.8104e+01,  1.1363e+01,  1.3107e+01,\n",
       "         2.7164e+01,  2.5847e+01,  4.5331e+00,  9.6068e+00,  8.9773e+00,\n",
       "         1.6884e+01, -2.5323e+01,  6.8000e+00,  8.2207e+00,  4.4176e+00,\n",
       "         4.8363e+00,  1.4533e+00,  8.6366e+00, -2.4815e+01, -4.0261e+00,\n",
       "         1.0514e+01,  2.7182e+00, -1.2455e+01,  2.3168e+01,  2.9243e+01,\n",
       "         2.8227e+01,  1.9269e+00,  3.6775e+01,  2.4843e+00, -1.9086e+01,\n",
       "        -1.5632e+01,  8.1745e+00,  1.1241e+01, -3.3501e+01,  5.3012e+00,\n",
       "         9.0995e+01, -1.2178e+01,  3.1808e+01,  4.9865e+00,  9.2430e+00,\n",
       "         2.3907e+01, -4.9372e+01, -7.3268e+00, -3.9993e+01,  1.5902e+01,\n",
       "         3.5550e+01,  1.3777e+01, -1.1832e+01, -2.2724e+01,  2.1123e+01,\n",
       "        -1.8979e+00, -4.0432e+01,  4.8710e+00,  3.1080e+00, -2.6201e+01,\n",
       "        -2.1869e+01,  1.5856e+01,  2.5940e+01,  8.2418e-01,  2.0395e+00,\n",
       "         6.5285e+00, -2.3024e+01,  2.4539e+00,  2.3121e+01, -4.1263e+01,\n",
       "        -2.0622e+01,  1.8547e+01, -4.6297e+00,  3.7699e+01,  2.4923e+01,\n",
       "        -2.8257e+01,  9.3758e+00, -1.5840e+01,  3.5827e+01,  7.0988e+01,\n",
       "        -5.7732e+00, -3.0637e+01,  1.0486e+02, -1.8046e+01,  1.1132e+01,\n",
       "         1.0698e+01, -3.0059e+01,  9.7454e+00, -8.3433e+00, -2.7425e+00,\n",
       "         1.7669e+01, -8.3259e+00,  1.1859e+01,  2.7234e+01,  2.5732e+01,\n",
       "         3.3019e-01, -9.6716e+00, -1.8577e+01, -1.6986e+00, -1.2178e+01,\n",
       "        -9.8333e+00, -8.0718e+00, -8.9959e+00, -8.3375e+00, -6.6467e+01,\n",
       "        -1.4327e+01, -2.1719e+01, -4.6730e+00,  2.7557e+00, -1.9525e+01,\n",
       "        -2.5739e+01, -8.5801e+00,  1.3719e+01, -2.6233e-01,  2.4808e+01,\n",
       "         6.2571e+00, -2.1095e+01,  2.0121e+00, -1.0266e+01,  2.5154e+01,\n",
       "        -2.9689e+01,  2.8712e+00,  4.0942e+00, -5.0250e+01,  3.0953e+01,\n",
       "         3.7491e+01, -2.0368e+01, -3.4515e+00, -7.4423e+00, -1.4055e+00,\n",
       "         2.4092e+00, -1.3483e+01,  5.4629e+00, -1.9917e+00, -2.6293e+01,\n",
       "         1.7542e+01, -1.8265e+01, -7.0091e+00,  1.3384e+01, -1.9605e+01,\n",
       "         4.1808e+00,  2.0107e+01,  2.9428e+01,  1.2548e+00,  3.5744e+00,\n",
       "        -2.9551e+01, -6.1486e+00,  2.5016e+00, -1.7376e+00, -2.8855e+00,\n",
       "        -1.4904e+01, -4.1194e+01, -1.6466e+00, -1.7824e+00, -3.5788e+01,\n",
       "         1.8490e+00, -1.1820e+01,  4.3174e+01,  1.0600e+01,  1.5140e+00,\n",
       "        -2.6487e+00, -8.7533e+00, -1.0012e+01,  2.1204e+00, -7.0438e+00,\n",
       "         9.7917e+00,  3.2798e+00,  6.1647e+00,  1.8559e+01,  1.4643e+01,\n",
       "         7.7702e+00, -1.2272e+00,  1.1588e+01, -2.0495e+01,  2.0569e+01,\n",
       "         3.6682e+01,  1.1495e+01, -1.1595e+01, -2.1419e+00, -1.2860e+01,\n",
       "        -1.2952e+01,  2.0153e+01, -1.3229e+01,  2.9451e+01,  2.3953e+01,\n",
       "        -1.7214e+01, -2.7518e+01,  2.5362e+01,  3.0618e+00, -6.0562e+00,\n",
       "        -5.7212e+00,  1.9668e+01,  4.0971e+00,  9.4740e+00, -3.8168e+01,\n",
       "         1.4112e+01,  1.1103e+01,  1.9714e+01, -1.3102e+00,  4.0546e-01,\n",
       "         9.9058e-01, -1.6498e+01, -7.2228e+00, -1.6140e+01, -1.1904e+00,\n",
       "         2.6448e+01, -1.9363e+01, -6.4171e-01, -3.8768e+01,  1.3476e+01,\n",
       "         2.4554e+01, -1.8762e+01,  2.3560e+01, -3.5465e+01,  3.3517e+01,\n",
       "        -1.0630e+01, -2.1361e+01,  1.5602e+01, -2.5854e+01, -1.3934e+01,\n",
       "         1.0381e+01,  3.0722e+01, -1.2236e+01,  1.3615e+01, -1.8878e+01,\n",
       "         2.2729e+01, -1.9940e+01,  4.9504e+01,  3.9870e+01, -2.7402e+01,\n",
       "        -1.1110e+01,  2.5085e+01, -7.3095e+00, -7.7959e-01, -1.4777e+00,\n",
       "        -6.6626e+00, -1.7249e+01,  2.4346e+01, -1.0087e+01, -1.4523e+01,\n",
       "         1.3234e+01, -1.8372e+00,  1.1773e+01, -2.5000e+01,  1.3234e+01,\n",
       "        -1.9421e+01,  1.2437e+01, -2.5046e+01,  2.6887e+01,  9.5780e+00,\n",
       "        -1.3203e+00,  2.0603e+01,  1.1721e+01, -2.5508e+01,  1.6306e+01,\n",
       "         6.9386e+00,  5.4283e+00, -5.7790e+00, -5.0943e+01,  1.3869e+01,\n",
       "        -3.5034e+00,  1.3846e+01, -8.6320e+00, -8.4877e+00,  2.6142e+00,\n",
       "        -3.7174e+01, -3.8884e+01,  2.3549e+01, -2.3660e+01, -3.7431e+00,\n",
       "         4.9865e+00,  3.9902e+00, -1.0370e+01,  2.9197e+01, -1.5193e+01,\n",
       "         1.7311e+01,  4.8536e+00, -1.9190e+01,  3.2827e+00,  4.3858e+00,\n",
       "        -7.8408e+00, -9.2616e+00,  6.0896e+00, -9.0377e-01,  1.4123e+01,\n",
       "         8.9773e+00,  1.6480e+01, -3.1945e+00, -1.8393e+01,  3.2455e+01,\n",
       "         1.6613e+00,  1.0566e+01,  3.6497e+01,  3.4418e+01, -1.7665e+01,\n",
       "         7.5623e+00, -1.9455e+01, -9.8853e+00, -1.0879e+00,  1.3530e+00,\n",
       "         9.4105e+00,  7.2562e+00,  9.2603e+00, -6.0851e+00,  7.0945e+00,\n",
       "         7.3863e-01,  1.1696e+02, -1.1618e+01, -1.1463e+00,  2.5778e+01,\n",
       "         9.2892e+00, -3.6990e+01, -1.5332e+01,  6.5227e+00, -1.4921e+00,\n",
       "        -7.1651e+00, -1.1543e+01,  3.5628e+00, -2.6340e+01,  2.7418e+01,\n",
       "         2.0193e+00, -1.1705e+01,  2.3298e+00,  2.0453e+01,  3.4164e+01,\n",
       "        -1.0272e+01,  7.6952e+00, -1.7468e+01,  4.3543e+01,  1.3603e+01,\n",
       "        -3.4933e+01, -6.8070e+00, -2.2193e+01, -2.0460e+01, -6.2814e+00,\n",
       "        -1.6495e+00, -3.2984e+00, -1.5470e+01, -3.4772e+01,  2.0730e+01,\n",
       "         7.9839e+00, -2.3660e+01, -1.0111e+01,  2.0927e+01, -1.6937e+01,\n",
       "        -6.9946e-01, -8.4010e+00, -2.6917e+01,  5.2521e+00, -1.1248e+01,\n",
       "         1.5833e+01,  5.3070e+00,  2.6194e+01, -2.4468e+01,  1.2887e+01,\n",
       "         1.7638e+00, -2.7957e+01, -1.6668e+00, -1.5597e+01, -1.1312e+01,\n",
       "         3.0583e+01,  8.0475e+00, -7.8235e+00, -2.5970e+01,  1.7381e+01,\n",
       "         3.4184e+00,  1.4331e+01, -1.1202e+01, -1.7272e+01, -1.3610e+01,\n",
       "         2.3814e+01, -7.9259e-01,  2.4069e+01, -1.6718e+01, -2.3498e+00,\n",
       "        -3.1306e+01,  9.6242e+00, -2.0541e+01, -2.9482e-01,  2.0511e+01,\n",
       "        -1.1416e+01,  1.0300e+01,  9.5722e+00, -3.8586e+00,  3.1669e+01,\n",
       "         1.5417e+01,  3.1946e+01, -2.4399e+01,  1.5117e+01,  3.1730e+00,\n",
       "         2.7164e+01,  6.1935e+00,  1.3465e+01, -1.8658e+01,  5.5323e+00,\n",
       "        -4.7847e+01, -7.8755e+00,  2.5524e+01, -3.2369e+01,  6.3033e+00,\n",
       "        -6.1717e+00,  1.7912e+00, -1.6152e+01,  1.0259e+01,  3.4415e+00,\n",
       "         7.5623e+00,  7.0368e+00, -1.1104e+01, -2.4907e+01,  2.5521e+00,\n",
       "         1.7935e+01,  1.0254e+01, -2.7316e-01,  2.4366e+00, -1.2354e-01,\n",
       "        -1.1797e+01, -7.4830e+01,  1.6457e+01,  2.2590e+01, -9.2759e-01,\n",
       "         2.5732e+01, -9.2847e+00, -1.7087e+00, -8.6205e+00,  2.0257e+01,\n",
       "        -1.0087e+01, -2.4307e+00, -2.7570e+00,  3.0921e+00,  2.1435e+01,\n",
       "        -7.0900e+00,  7.6894e+00,  3.1441e+00, -1.0149e+00, -2.7818e+01,\n",
       "        -2.0252e+01, -2.0483e+01, -3.3793e+00, -3.3633e-01,  9.7454e+00,\n",
       "        -1.1450e+01, -2.2242e+00, -2.9412e+01, -1.4246e+01,  1.6815e+01,\n",
       "         3.1438e+01, -8.9959e+00,  3.8285e+00, -1.7850e+01, -1.3657e+01,\n",
       "         1.4054e+01, -1.2777e+00,  1.4839e+01, -5.7443e+00, -1.6648e+01,\n",
       "        -1.2594e+01, -1.4249e-01,  6.9212e+00, -7.2402e+00,  5.2183e+01,\n",
       "        -1.7030e+01, -4.5328e-01, -1.6002e+01,  2.5801e+01, -1.3709e+00,\n",
       "         9.1261e-01,  2.0303e+01,  2.3734e+01, -5.4382e+00, -1.4742e+01,\n",
       "        -3.8976e+01, -1.8473e+01,  3.3032e+01, -9.2500e+00, -4.4506e+00,\n",
       "         9.9938e+00, -6.0966e+00, -3.6161e+00, -1.0438e+00,  4.7439e+00,\n",
       "         1.1871e+01, -2.0876e+01,  2.6084e+00, -1.5874e+01, -5.9060e+00,\n",
       "         1.6396e+00, -3.1714e+00, -1.5874e+01,  1.4943e+01, -1.2171e+00,\n",
       "        -2.0509e+00,  7.3255e+00, -9.4522e+00, -2.6293e+01, -2.2877e+00,\n",
       "        -5.3603e+00,  2.3933e+00, -1.9802e+01, -1.8208e+01,  1.5299e+00,\n",
       "        -1.2675e+01, -5.1061e+00, -9.2096e+00,  1.5001e+01, -1.3599e+01,\n",
       "         9.5260e+00,  5.2724e+00,  8.5522e-01, -4.4267e+01,  3.6821e+01,\n",
       "        -1.9542e+00, -1.4766e+01,  1.3689e+00, -4.4174e+01, -4.5892e+00,\n",
       "         1.6041e+01, -4.2673e+01,  1.6872e+01,  1.4886e+01, -2.7945e+00,\n",
       "         3.4589e+00,  1.7716e+01,  6.9963e+00,  6.0001e+00,  6.1011e+00,\n",
       "         1.8755e+01, -1.3030e+00,  1.3268e+01, -3.6190e+00, -4.1330e+00,\n",
       "         9.4336e+00, -1.2813e+00, -1.3749e+01,  1.0311e+01, -3.1861e+01,\n",
       "         2.4854e+01,  6.1762e+00, -8.8169e+00,  7.3413e-02,  5.3503e+00,\n",
       "         2.7603e+01,  5.7728e+01,  1.3257e+01, -3.3455e+01, -5.0744e+00,\n",
       "        -9.9835e+00, -4.3758e+01, -2.4076e+01,  1.2575e+01,  7.7414e+00,\n",
       "         4.2689e+01, -8.9843e+00,  5.8701e+00, -1.2171e+00,  8.0128e+00,\n",
       "         1.6630e+01,  1.2321e+01,  9.8667e+00, -2.0737e+01,  1.5498e+01,\n",
       "        -3.9392e+01,  2.0107e+01,  6.5028e+01,  2.0823e+01, -6.6625e-01,\n",
       "        -1.3016e+00, -1.7047e-01, -2.7316e-01,  7.7240e+00, -2.0599e+01,\n",
       "        -3.3663e+01,  1.1905e+01, -1.0867e+01,  9.3065e+00,  2.7927e+01,\n",
       "        -8.5612e-01,  1.2599e+01, -4.1763e+00,  1.5070e+01,  7.7272e+01,\n",
       "        -2.9343e+01, -1.3137e+01,  2.0592e+01,  2.0719e+01,  3.3633e+01,\n",
       "        -1.3807e+01,  1.2483e+01, -2.1015e+01, -3.1399e+01, -2.7703e+01,\n",
       "         5.8701e+00, -3.3417e+00, -4.7307e+00, -5.6317e+00, -1.1578e+01,\n",
       "         2.5391e+00, -1.2571e+01, -2.5392e+01,  5.4687e+00, -1.6533e+01,\n",
       "        -5.9522e+00, -2.6109e+01, -8.1816e+00, -3.1829e+00, -2.4645e-01,\n",
       "        -5.3378e-01, -1.5516e+01,  7.5046e+00, -1.8329e+00, -1.8878e+01,\n",
       "         6.0607e+00, -1.8924e+01, -2.0235e+00,  1.3396e-01,  1.3638e+01,\n",
       "         2.7187e+01,  1.3315e+01, -1.2606e+01, -2.5346e+01, -2.8075e+00,\n",
       "        -3.7836e+00, -3.6504e+01, -1.9143e+01,  2.1978e+01, -2.2181e+01,\n",
       "         3.6636e+01, -6.7897e+00, -3.8815e+01, -7.1766e+00, -4.5488e+00,\n",
       "         1.2044e+01, -2.4515e+01, -3.2115e+01], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hook_model.embed_weights[33348,:]\n",
    "embedded_tokens[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9203)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate two random 758 dimensional vectors\n",
    "# find their cosine similarity\n",
    "random_vec1 = th.randn(758)\n",
    "random_vec2 = th.randn(758)\n",
    "cosine_similarity = th.nn.CosineSimilarity(dim=0)\n",
    "1-abs(cosine_similarity(random_vec1, random_vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.0810, grad_fn=<MinBackward1>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.norm(hook_model.embed_weights@embedded_tokens[0,i,:]).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.6949e-06, grad_fn=<MinBackward1>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ew = hook_model.embed_weights\n",
    "e_norm = ew /ew.norm(dim=1).unsqueeze(1)\n",
    "t_size = embedded_tokens[0,:,:].norm(dim=1)\n",
    "t_norm = embedded_tokens[0,0,:] / embedded_tokens[0,0,:].norm()\n",
    "abs((e_norm@t_norm.T)).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0581, -0.0240,  0.0173,  0.0930, -0.0442,  0.0115,  0.0262, -0.0446,\n",
       "         0.0297,  0.0895], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = ew[1]\n",
    "s_norm = s / s.norm()\n",
    "s_norm[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8182, 0.9675, 0.9761, 0.9691, 0.9824, 0.8661, 0.4779, 0.9808, 0.9621],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor(0.9675, grad_fn=<NormBackward1>)\n"
     ]
    }
   ],
   "source": [
    "t_norm = embedded_tokens[0,:,:].norm(dim=1)\n",
    "print(t_norm[:])\n",
    "print(embedded_tokens[0,1,:].norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\logan\\AppData\\Local\\Temp\\ipykernel_11792\\2094925361.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\logan\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_11792\\\\2094925361.py'</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'builtin_function_or_method'</span> object is not subscriptable\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\logan\\AppData\\Local\\Temp\\ipykernel_11792\\2094925361.py\u001b[0m:\u001b[94m2\u001b[0m in \u001b[92m<module>\u001b[0m                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m'C:\\\\Users\\\\logan\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_11792\\\\2094925361.py'\u001b[0m                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mTypeError: \u001b[0m\u001b[32m'builtin_function_or_method'\u001b[0m object is not subscriptable\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_tokens = th.tensor[[(hook_model.embed_weights@embedded_tokens[0,i,:]).argmax().item() for i in range(seq)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hey there partner']\n"
     ]
    }
   ],
   "source": [
    "text = \"hey there partner\"\n",
    "tokens = model.to_tokens(text, prepend_bos=False)\n",
    "text_again = model.to_string(tokens)\n",
    "print(text_again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x16d348e1430>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.gpt_neox.layers[0].mlp.dense_h_to_4h.register_forward_hook(lambda m, i, o: print(o[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('self', 'input', 'prepend_bos', 'move_to_device', 'truncate', 'tokens')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_tokens.__code__.co_varnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = T(model)\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "tokens = tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['hook_embed', 'blocks.0.hook_resid_pre', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_rot_q', 'blocks.0.attn.hook_rot_k', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_z', 'blocks.0.hook_attn_out', 'blocks.0.ln2.hook_scale', 'blocks.0.ln2.hook_normalized', 'blocks.0.mlp.hook_pre', 'blocks.0.mlp.hook_post', 'blocks.0.hook_mlp_out', 'blocks.0.hook_resid_post', 'blocks.1.hook_resid_pre', 'blocks.1.ln1.hook_scale', 'blocks.1.ln1.hook_normalized', 'blocks.1.attn.hook_q', 'blocks.1.attn.hook_k', 'blocks.1.attn.hook_v', 'blocks.1.attn.hook_rot_q', 'blocks.1.attn.hook_rot_k', 'blocks.1.attn.hook_attn_scores', 'blocks.1.attn.hook_pattern', 'blocks.1.attn.hook_z', 'blocks.1.hook_attn_out', 'blocks.1.ln2.hook_scale', 'blocks.1.ln2.hook_normalized', 'blocks.1.mlp.hook_pre', 'blocks.1.mlp.hook_post', 'blocks.1.hook_mlp_out', 'blocks.1.hook_resid_post', 'blocks.2.hook_resid_pre', 'blocks.2.ln1.hook_scale', 'blocks.2.ln1.hook_normalized', 'blocks.2.attn.hook_q', 'blocks.2.attn.hook_k', 'blocks.2.attn.hook_v', 'blocks.2.attn.hook_rot_q', 'blocks.2.attn.hook_rot_k', 'blocks.2.attn.hook_attn_scores', 'blocks.2.attn.hook_pattern', 'blocks.2.attn.hook_z', 'blocks.2.hook_attn_out', 'blocks.2.ln2.hook_scale', 'blocks.2.ln2.hook_normalized', 'blocks.2.mlp.hook_pre', 'blocks.2.mlp.hook_post', 'blocks.2.hook_mlp_out', 'blocks.2.hook_resid_post', 'blocks.3.hook_resid_pre', 'blocks.3.ln1.hook_scale', 'blocks.3.ln1.hook_normalized', 'blocks.3.attn.hook_q', 'blocks.3.attn.hook_k', 'blocks.3.attn.hook_v', 'blocks.3.attn.hook_rot_q', 'blocks.3.attn.hook_rot_k', 'blocks.3.attn.hook_attn_scores', 'blocks.3.attn.hook_pattern', 'blocks.3.attn.hook_z', 'blocks.3.hook_attn_out', 'blocks.3.ln2.hook_scale', 'blocks.3.ln2.hook_normalized', 'blocks.3.mlp.hook_pre', 'blocks.3.mlp.hook_post', 'blocks.3.hook_mlp_out', 'blocks.3.hook_resid_post', 'blocks.4.hook_resid_pre', 'blocks.4.ln1.hook_scale', 'blocks.4.ln1.hook_normalized', 'blocks.4.attn.hook_q', 'blocks.4.attn.hook_k', 'blocks.4.attn.hook_v', 'blocks.4.attn.hook_rot_q', 'blocks.4.attn.hook_rot_k', 'blocks.4.attn.hook_attn_scores', 'blocks.4.attn.hook_pattern', 'blocks.4.attn.hook_z', 'blocks.4.hook_attn_out', 'blocks.4.ln2.hook_scale', 'blocks.4.ln2.hook_normalized', 'blocks.4.mlp.hook_pre', 'blocks.4.mlp.hook_post', 'blocks.4.hook_mlp_out', 'blocks.4.hook_resid_post', 'blocks.5.hook_resid_pre', 'blocks.5.ln1.hook_scale', 'blocks.5.ln1.hook_normalized', 'blocks.5.attn.hook_q', 'blocks.5.attn.hook_k', 'blocks.5.attn.hook_v', 'blocks.5.attn.hook_rot_q', 'blocks.5.attn.hook_rot_k', 'blocks.5.attn.hook_attn_scores', 'blocks.5.attn.hook_pattern', 'blocks.5.attn.hook_z', 'blocks.5.hook_attn_out', 'blocks.5.ln2.hook_scale', 'blocks.5.ln2.hook_normalized', 'blocks.5.mlp.hook_pre', 'blocks.5.mlp.hook_post', 'blocks.5.hook_mlp_out', 'blocks.5.hook_resid_post', 'blocks.6.hook_resid_pre', 'blocks.6.ln1.hook_scale', 'blocks.6.ln1.hook_normalized', 'blocks.6.attn.hook_q', 'blocks.6.attn.hook_k', 'blocks.6.attn.hook_v', 'blocks.6.attn.hook_rot_q', 'blocks.6.attn.hook_rot_k', 'blocks.6.attn.hook_attn_scores', 'blocks.6.attn.hook_pattern', 'blocks.6.attn.hook_z', 'blocks.6.hook_attn_out', 'blocks.6.ln2.hook_scale', 'blocks.6.ln2.hook_normalized', 'blocks.6.mlp.hook_pre', 'blocks.6.mlp.hook_post', 'blocks.6.hook_mlp_out', 'blocks.6.hook_resid_post', 'blocks.7.hook_resid_pre', 'blocks.7.ln1.hook_scale', 'blocks.7.ln1.hook_normalized', 'blocks.7.attn.hook_q', 'blocks.7.attn.hook_k', 'blocks.7.attn.hook_v', 'blocks.7.attn.hook_rot_q', 'blocks.7.attn.hook_rot_k', 'blocks.7.attn.hook_attn_scores', 'blocks.7.attn.hook_pattern', 'blocks.7.attn.hook_z', 'blocks.7.hook_attn_out', 'blocks.7.ln2.hook_scale', 'blocks.7.ln2.hook_normalized', 'blocks.7.mlp.hook_pre', 'blocks.7.mlp.hook_post', 'blocks.7.hook_mlp_out', 'blocks.7.hook_resid_post', 'blocks.8.hook_resid_pre', 'blocks.8.ln1.hook_scale', 'blocks.8.ln1.hook_normalized', 'blocks.8.attn.hook_q', 'blocks.8.attn.hook_k', 'blocks.8.attn.hook_v', 'blocks.8.attn.hook_rot_q', 'blocks.8.attn.hook_rot_k', 'blocks.8.attn.hook_attn_scores', 'blocks.8.attn.hook_pattern', 'blocks.8.attn.hook_z', 'blocks.8.hook_attn_out', 'blocks.8.ln2.hook_scale', 'blocks.8.ln2.hook_normalized', 'blocks.8.mlp.hook_pre', 'blocks.8.mlp.hook_post', 'blocks.8.hook_mlp_out', 'blocks.8.hook_resid_post', 'blocks.9.hook_resid_pre', 'blocks.9.ln1.hook_scale', 'blocks.9.ln1.hook_normalized', 'blocks.9.attn.hook_q', 'blocks.9.attn.hook_k', 'blocks.9.attn.hook_v', 'blocks.9.attn.hook_rot_q', 'blocks.9.attn.hook_rot_k', 'blocks.9.attn.hook_attn_scores', 'blocks.9.attn.hook_pattern', 'blocks.9.attn.hook_z', 'blocks.9.hook_attn_out', 'blocks.9.ln2.hook_scale', 'blocks.9.ln2.hook_normalized', 'blocks.9.mlp.hook_pre', 'blocks.9.mlp.hook_post', 'blocks.9.hook_mlp_out', 'blocks.9.hook_resid_post', 'blocks.10.hook_resid_pre', 'blocks.10.ln1.hook_scale', 'blocks.10.ln1.hook_normalized', 'blocks.10.attn.hook_q', 'blocks.10.attn.hook_k', 'blocks.10.attn.hook_v', 'blocks.10.attn.hook_rot_q', 'blocks.10.attn.hook_rot_k', 'blocks.10.attn.hook_attn_scores', 'blocks.10.attn.hook_pattern', 'blocks.10.attn.hook_z', 'blocks.10.hook_attn_out', 'blocks.10.ln2.hook_scale', 'blocks.10.ln2.hook_normalized', 'blocks.10.mlp.hook_pre', 'blocks.10.mlp.hook_post', 'blocks.10.hook_mlp_out', 'blocks.10.hook_resid_post', 'blocks.11.hook_resid_pre', 'blocks.11.ln1.hook_scale', 'blocks.11.ln1.hook_normalized', 'blocks.11.attn.hook_q', 'blocks.11.attn.hook_k', 'blocks.11.attn.hook_v', 'blocks.11.attn.hook_rot_q', 'blocks.11.attn.hook_rot_k', 'blocks.11.attn.hook_attn_scores', 'blocks.11.attn.hook_pattern', 'blocks.11.attn.hook_z', 'blocks.11.hook_attn_out', 'blocks.11.ln2.hook_scale', 'blocks.11.ln2.hook_normalized', 'blocks.11.mlp.hook_pre', 'blocks.11.mlp.hook_post', 'blocks.11.hook_mlp_out', 'blocks.11.hook_resid_post', 'ln_final.hook_scale', 'ln_final.hook_normalized'])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets.\"\n",
    "tokens = model.to_tokens(text)\n",
    "_, cache = model.run_with_cache(tokens, remove_batch_dim=True)\n",
    "cache.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embed(tokens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\logan\\AppData\\Local\\Temp\\ipykernel_18472\\1902589174.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">9</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\logan\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_18472\\\\1902589174.py'</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\logan\\miniconda3\\lib\\site-packages\\transformer_lens\\HookedTransformer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">363</span> in         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">run_with_cache</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 360 </span><span style=\"color: #bfbfbf; text-decoration-color: #bfbfbf\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 361 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">Wrapper around run_with_cache in HookedRootModule. If return_cache_object is Tru</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 362 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 363 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>out, cache_dict = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">super</span>().run_with_cache(                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 364 │   │   │   </span>*model_args, remove_batch_dim=remove_batch_dim, **kwargs                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 365 │   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 366 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> return_cache_object:                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\logan\\miniconda3\\lib\\site-packages\\transformer_lens\\hook_points.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">277</span> in               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">run_with_cache</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">274 │   │   </span>cache_dict = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.add_caching_hooks(                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">275 │   │   │   </span>names_filter, incl_bwd, device, remove_batch_dim=remove_batch_dim              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">276 │   │   </span>)                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>277 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>model_out = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>(*model_args, **model_kwargs)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">278 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">279 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> incl_bwd:                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">280 │   │   │   </span>model_out.backward()                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\logan\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1194</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1191 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1192 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1193 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1194 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1195 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1196 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1197 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\logan\\miniconda3\\lib\\site-packages\\transformer_lens\\HookedTransformer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">263</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 260 │   │   │   </span>pos_offset = cache_ctx_length                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 261 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.cfg.use_hook_tokens:                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 262 │   │   │   </span>tokens = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.hook_tokens(tokens)                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 263 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>embed = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.hook_embed(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.embed(tokens))  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># [batch, pos, d_model]</span>              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 264 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.cfg.positional_embedding_type == <span style=\"color: #808000; text-decoration-color: #808000\">\"standard\"</span>:                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 265 │   │   │   </span>pos_embed = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.hook_pos_embed(                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 266 │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.pos_embed(tokens, pos_offset)                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\logan\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1194</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1191 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1192 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1193 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1194 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1195 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1196 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1197 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\logan\\miniconda3\\lib\\site-packages\\transformer_lens\\components.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">42</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 39 │   </span>) -&gt; TT[T.batch, T.pos, T.d_model]:                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 40 │   │   # If A has shape [a, b] and B has shape [c, d], then A[:, B] has shape [a, c, d]</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 41 │   │   # B acts as a tensor of indices into the second dimension (so &gt;=0 and &lt;b)</span>          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 42 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.W_E[tokens, :]                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 43 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 44 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 45 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">class</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; text-decoration: underline\">Unembed</span>(nn.Module):                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">IndexError: </span>tensors used as indices must be long, byte or bool tensors\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\logan\\AppData\\Local\\Temp\\ipykernel_18472\\1902589174.py\u001b[0m:\u001b[94m9\u001b[0m in \u001b[92m<module>\u001b[0m                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m'C:\\\\Users\\\\logan\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_18472\\\\1902589174.py'\u001b[0m                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\logan\\miniconda3\\lib\\site-packages\\transformer_lens\\HookedTransformer.py\u001b[0m:\u001b[94m363\u001b[0m in         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mrun_with_cache\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 360 \u001b[0m\u001b[2;90m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 361 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33mWrapper around run_with_cache in HookedRootModule. If return_cache_object is Tru\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 362 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 363 \u001b[2m│   │   \u001b[0mout, cache_dict = \u001b[96msuper\u001b[0m().run_with_cache(                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 364 \u001b[0m\u001b[2m│   │   │   \u001b[0m*model_args, remove_batch_dim=remove_batch_dim, **kwargs                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 365 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 366 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m return_cache_object:                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\logan\\miniconda3\\lib\\site-packages\\transformer_lens\\hook_points.py\u001b[0m:\u001b[94m277\u001b[0m in               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mrun_with_cache\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m274 \u001b[0m\u001b[2m│   │   \u001b[0mcache_dict = \u001b[96mself\u001b[0m.add_caching_hooks(                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m275 \u001b[0m\u001b[2m│   │   │   \u001b[0mnames_filter, incl_bwd, device, remove_batch_dim=remove_batch_dim              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m276 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m277 \u001b[2m│   │   \u001b[0mmodel_out = \u001b[96mself\u001b[0m(*model_args, **model_kwargs)                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m278 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m279 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m incl_bwd:                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m280 \u001b[0m\u001b[2m│   │   │   \u001b[0mmodel_out.backward()                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\logan\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m:\u001b[94m1194\u001b[0m in \u001b[92m_call_impl\u001b[0m        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1191 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1192 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1193 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1194 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1195 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1196 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1197 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\logan\\miniconda3\\lib\\site-packages\\transformer_lens\\HookedTransformer.py\u001b[0m:\u001b[94m263\u001b[0m in \u001b[92mforward\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 260 \u001b[0m\u001b[2m│   │   │   \u001b[0mpos_offset = cache_ctx_length                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 261 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.cfg.use_hook_tokens:                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 262 \u001b[0m\u001b[2m│   │   │   \u001b[0mtokens = \u001b[96mself\u001b[0m.hook_tokens(tokens)                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 263 \u001b[2m│   │   \u001b[0membed = \u001b[96mself\u001b[0m.hook_embed(\u001b[96mself\u001b[0m.embed(tokens))  \u001b[2m# [batch, pos, d_model]\u001b[0m              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 264 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.cfg.positional_embedding_type == \u001b[33m\"\u001b[0m\u001b[33mstandard\u001b[0m\u001b[33m\"\u001b[0m:                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 265 \u001b[0m\u001b[2m│   │   │   \u001b[0mpos_embed = \u001b[96mself\u001b[0m.hook_pos_embed(                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 266 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.pos_embed(tokens, pos_offset)                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\logan\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m:\u001b[94m1194\u001b[0m in \u001b[92m_call_impl\u001b[0m        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1191 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1192 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1193 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1194 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1195 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1196 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1197 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\logan\\miniconda3\\lib\\site-packages\\transformer_lens\\components.py\u001b[0m:\u001b[94m42\u001b[0m in \u001b[92mforward\u001b[0m         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 39 \u001b[0m\u001b[2m│   \u001b[0m) -> TT[T.batch, T.pos, T.d_model]:                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 40 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# If A has shape [a, b] and B has shape [c, d], then A[:, B] has shape [a, c, d]\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 41 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# B acts as a tensor of indices into the second dimension (so >=0 and <b)\u001b[0m          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 42 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.W_E[tokens, :]                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 43 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 44 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 45 \u001b[0m\u001b[94mclass\u001b[0m \u001b[4;92mUnembed\u001b[0m(nn.Module):                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mIndexError: \u001b[0mtensors used as indices must be long, byte or bool tensors\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layer = 5\n",
    "neuron = 0\n",
    "model.requires_grad = False\n",
    "\n",
    "embedded_tokens = th.nn.Parameter(model.embed(tokens))\n",
    "embedded_tokens.requires_grad = True\n",
    "\n",
    "# Run the model\n",
    "_, cache = model.run_with_cache(embedded_tokens, remove_batch_dim=True)\n",
    "\n",
    "# Get the neuron's max value\n",
    "cache[f'blocks.{layer}.mlp.hook_post'][0,neuron].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False)\n",
      "tensor(False)\n"
     ]
    }
   ],
   "source": [
    "# import Gelu from Functional\n",
    "from torch.nn.functional import gelu\n",
    "init_ln = model.blocks[0].ln1(embedded_tokens)\n",
    "attn =  model.blocks[0].attn(init_ln)\n",
    "mlp = model.blocks[0].mlp(init_ln)\n",
    "serial_attn_then_mlp = model.blocks[0].mlp(attn)\n",
    "add_attn_mlp = attn + mlp\n",
    "actual = model.blocks[0](embedded_tokens)\n",
    "ln = model.blocks[0].ln2(add_attn_mlp)\n",
    "# Check that the serial_attn_then_mlp is the same as the actual\n",
    "print((th.abs(serial_attn_then_mlp - actual) < 1e-5).all())\n",
    "# Check that the add_attn_mlp is the same as the actual\n",
    "print((th.abs(add_attn_mlp - actual) < 1e-5).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x16d36bde430>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 768])\n",
      "torch.Size([32, 768])\n",
      "torch.Size([32, 768])\n",
      "tensor(False)\n"
     ]
    }
   ],
   "source": [
    "original\n",
    "input_layernorm = original.gpt_neox.layers[0].input_layernorm\n",
    "post_attention_layernorm = original.gpt_neox.layers[0].post_attention_layernorm\n",
    "attention = original.gpt_neox.layers[0].attention\n",
    "mlp = original.gpt_neox.layers[0].mlp\n",
    "full_first_layer = original.gpt_neox.layers[0]\n",
    "\n",
    "il = input_layernorm(embedded_tokens)\n",
    "attn_pre = attention(il, attention_mask=None)\n",
    "attn_act =  post_attention_layernorm(attn_pre[0])\n",
    "mlp_act = mlp(il)\n",
    "out = mlp_act+attn_act\n",
    "actual = full_first_layer(embedded_tokens)\n",
    "# Check if out is same as actual\n",
    "print((th.abs(out[0] - actual[0][0]) < 1e-5).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0): GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "      (1): GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "      (2): GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "      (3): GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "      (4): GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "      (5): GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "      (6): GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "      (7): GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "      (8): GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "      (9): GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "      (10): GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "      (11): GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=768, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1854, -0.1584,  0.5147], grad_fn=<SliceBackward0>)\n",
      "tensor([-0.0284,  0.0493,  0.1089], grad_fn=<SliceBackward0>)\n",
      "tensor([-0.1896, -0.1550,  0.5029], grad_fn=<SliceBackward0>)\n",
      "tensor([-0.3918, -0.3203,  1.0390], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(actual[0,0,:3])\n",
    "print(serial_attn_then_mlp[0,0,:3])\n",
    "print(add_attn_mlp[0,0,:3])\n",
    "print(ln[0,0,:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1854, -0.1584,  0.5147,  ..., -0.1457,  0.5370,  0.1068],\n",
       "         [-0.3354, -0.3593,  0.0724,  ..., -0.3656, -0.2987, -0.0381],\n",
       "         [-0.3607, -0.2062,  0.1508,  ..., -0.2275,  0.1065, -0.3098],\n",
       "         ...,\n",
       "         [ 0.1859, -0.4723,  0.4314,  ..., -0.0177,  0.2272,  0.2222],\n",
       "         [-0.3957, -0.3710, -0.2304,  ..., -0.0337,  0.3666,  0.1840],\n",
       "         [ 0.0503,  0.6451,  0.2918,  ..., -0.0455, -0.3018,  0.0271]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.blocks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\logan\\AppData\\Local\\Temp\\ipykernel_18472\\2734888106.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\logan\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_18472\\\\2734888106.py'</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AttributeError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Parameter'</span> object has no attribute <span style=\"color: #008000; text-decoration-color: #008000\">'register_forward_hook'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\logan\\AppData\\Local\\Temp\\ipykernel_18472\\2734888106.py\u001b[0m:\u001b[94m1\u001b[0m in \u001b[92m<module>\u001b[0m                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m'C:\\\\Users\\\\logan\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_18472\\\\2734888106.py'\u001b[0m                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mAttributeError: \u001b[0m\u001b[32m'Parameter'\u001b[0m object has no attribute \u001b[32m'register_forward_hook'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.blocks[0].mlp.W_in.register_forward_hook(lambda m, i, o: print(o[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2488,  0.8720, -0.7121,  ..., -0.2444,  0.9457,  0.3559],\n",
       "         [ 0.5306, -0.3194, -0.6743,  ...,  0.2470,  0.6080, -0.1530],\n",
       "         [ 0.1681,  0.5646, -0.6971,  ..., -0.0104,  0.3981,  0.1245],\n",
       "         ...,\n",
       "         [ 0.3086,  0.5555, -0.0274,  ..., -0.4727,  0.5518, -0.0732],\n",
       "         [ 0.9248,  1.5012, -0.4375,  ...,  0.0385,  0.9525, -0.2166],\n",
       "         [ 0.4605, -1.1019,  5.6312,  ..., -1.2397,  3.0214, -2.3836]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_blocks = [mod for mod in (list(model.children())[2] if hasattr(list(model.children())[i], \"__iter__\") else [list(model.children())[i]])]\n",
    "embedding_matrix = th.nn.Sequential(*(transformer_blocks[:layer +1])).requires_grad_(False)\n",
    "embedding_matrix(embedded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 3072])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.blocks[0].mlp.W_in"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c3ce88656189d071683562ae2c1ba9e660ff5487e9c26da9e3beecfb49e6cd8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
